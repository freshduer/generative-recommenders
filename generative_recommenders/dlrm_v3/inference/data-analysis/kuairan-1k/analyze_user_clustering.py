#!/usr/bin/env python3
"""åˆ†æç”¨æˆ·ä¹‹é—´çš„itemé‡åˆåº¦å’Œèšç±»æ•ˆåº”ï¼Œç”¨äºGPUè°ƒåº¦ä¼˜åŒ–ã€‚"""
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp
from datetime import datetime
import time

import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import TruncatedSVD
from sklearn.manifold import TSNE

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    from matplotlib.ticker import FuncFormatter
except Exception:
    plt = None
    sns = None
    FuncFormatter = None

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None


def _load_single_csv(csv_path: Path) -> pd.DataFrame:
    """åŠ è½½å•ä¸ªCSVæ–‡ä»¶ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰"""
    usecols = ["user_id", "video_id"]
    try:
        # å°è¯•ä½¿ç”¨pyarrowå¼•æ“ï¼ˆæ›´å¿«ï¼Œå¦‚æœå¯ç”¨ï¼‰
        try:
            frame = pd.read_csv(
                csv_path,
                usecols=usecols,
                dtype={
                    "user_id": "int32",  # ä½¿ç”¨int32èŠ‚çœå†…å­˜å’ŒåŠ é€Ÿ
                    "video_id": "int32",
                },
                engine="pyarrow",
                na_filter=False,
            )
        except (ImportError, ValueError, TypeError):
            # å›é€€åˆ°Cå¼•æ“
            frame = pd.read_csv(
                csv_path,
                usecols=usecols,
                dtype={
                    "user_id": "int32",
                    "video_id": "int32",
                },
                engine="c",
                low_memory=False,
                na_filter=False,
            )
    except KeyError:
        # å¦‚æœæŸäº›æ–‡ä»¶æ²¡æœ‰è¿™äº›åˆ—ï¼Œå°è¯•è¯»å–æ‰€æœ‰åˆ—
        frame = pd.read_csv(
            csv_path,
            dtype={
                "user_id": "int32",
                "video_id": "int32",
            },
            engine="c",
            low_memory=False,
        )
        if "user_id" in frame.columns and "video_id" in frame.columns:
            frame = frame[["user_id", "video_id"]]
        else:
            raise ValueError(f"æ–‡ä»¶ {csv_path} ç¼ºå°‘å¿…éœ€çš„åˆ—: user_id, video_id")
    
    return frame


def _load_log_frames(
    data_dir: Path, show_progress: bool, n_workers: Optional[int] = None
) -> Tuple[pd.DataFrame, List[Path]]:
    """åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰KuaiRandæ—¥å¿—CSVæ–‡ä»¶ï¼ˆå¤šè¿›ç¨‹ä¼˜åŒ–ç‰ˆæœ¬ï¼‰ã€‚"""
    log_paths = sorted(data_dir.glob("log_standard*.csv"))
    if not log_paths:
        msg = (
            "æœªæ‰¾åˆ°KuaiRandæ—¥å¿—æ–‡ä»¶ã€‚æœŸæœ›æ–‡ä»¶æ ¼å¼ï¼š"
            "'log_standard_4_08_to_4_21_1k.csv'ã€‚"
        )
        raise FileNotFoundError(msg)

    # ç¡®å®šå·¥ä½œè¿›ç¨‹æ•°
    if n_workers is None:
        n_workers = min(mp.cpu_count(), len(log_paths), 4)  # å‡å°‘åˆ°æœ€å¤š4ä¸ªè¿›ç¨‹ï¼Œé¿å…å†…å­˜é—®é¢˜
    
    frames = []
    if n_workers > 1 and len(log_paths) > 1:
        # å¤šè¿›ç¨‹å¹¶è¡Œè¯»å–ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œé‡è¯•ï¼‰
        if show_progress:
            print(f"å°è¯•ä½¿ç”¨ {n_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œè¯»å– {len(log_paths)} ä¸ªæ–‡ä»¶...")
        
        try:
            with ProcessPoolExecutor(max_workers=n_workers) as executor:
                future_to_path = {
                    executor.submit(_load_single_csv, path): path 
                    for path in log_paths
                }
                
                iter_futures = future_to_path.items()
                if show_progress and tqdm is not None:
                    iter_futures = tqdm(future_to_path.items(), desc="è¯»å–æ—¥å¿—", unit="æ–‡ä»¶", total=len(log_paths))
                
                failed_paths = []
                for future_item in iter_futures:
                    if isinstance(future_item, tuple):
                        future, path = future_item
                    else:
                        future = future_item
                        path = future_to_path[future]
                    
                    try:
                        frame = future.result(timeout=600)  # 10åˆ†é’Ÿè¶…æ—¶
                        frames.append(frame)
                    except Exception as e:
                        if show_progress:
                            print(f"è­¦å‘Š: è¯»å–æ–‡ä»¶ {path.name} å¤±è´¥: {e}")
                        failed_paths.append(path)
            
            # å¦‚æœå¤šè¿›ç¨‹è¯»å–å¤±è´¥å¤ªå¤šï¼Œå›é€€åˆ°å•è¿›ç¨‹
            if len(frames) == 0:
                if show_progress:
                    print(f"æ‰€æœ‰æ–‡ä»¶å¤šè¿›ç¨‹è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°å•è¿›ç¨‹è¯»å–...")
                n_workers = 1
            elif len(failed_paths) > 0:
                if show_progress:
                    print(f"{len(failed_paths)} ä¸ªæ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå°è¯•å•è¿›ç¨‹é‡æ–°è¯»å–...")
                # å•è¿›ç¨‹é‡è¯•å¤±è´¥çš„æ–‡ä»¶
                for path in failed_paths:
                    try:
                        frame = _load_single_csv(path)
                        frames.append(frame)
                        if show_progress:
                            print(f"æˆåŠŸé‡æ–°è¯»å–æ–‡ä»¶: {path.name}")
                    except Exception as e:
                        if show_progress:
                            print(f"é”™è¯¯: é‡æ–°è¯»å–æ–‡ä»¶ {path.name} ä»ç„¶å¤±è´¥: {e}")
                        raise RuntimeError(f"æ— æ³•è¯»å–æ–‡ä»¶ {path.name}: {e}")
        except Exception as e:
            if show_progress:
                print(f"å¤šè¿›ç¨‹è¯»å–å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
                print(f"å›é€€åˆ°å•è¿›ç¨‹è¯»å–...")
            n_workers = 1
            frames = []
    # å•è¿›ç¨‹è¯»å–ï¼ˆå¦‚æœå¤šè¿›ç¨‹å¤±è´¥æˆ–æœªå¯ç”¨ï¼‰
    if n_workers == 1 or len(frames) == 0:
        if show_progress:
            print(f"ä½¿ç”¨å•è¿›ç¨‹è¯»å– {len(log_paths)} ä¸ªæ–‡ä»¶...")
        iter_paths = log_paths
        if show_progress and tqdm is not None:
            iter_paths = tqdm(log_paths, desc="è¯»å–æ—¥å¿—", unit="æ–‡ä»¶")
        elif show_progress and tqdm is None:
            print("tqdmæœªå®‰è£…ï¼›ç»§ç»­æ‰§è¡Œä½†ä¸æ˜¾ç¤ºè¿›åº¦æ¡ã€‚")
        
        frames = []
        for csv_path in iter_paths:
            try:
                frame = _load_single_csv(csv_path)
                frames.append(frame)
            except Exception as e:
                if show_progress:
                    print(f"é”™è¯¯: è¯»å–æ–‡ä»¶ {csv_path.name} å¤±è´¥: {e}")
                raise RuntimeError(f"æ— æ³•è¯»å–æ–‡ä»¶ {csv_path.name}: {e}")

    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    if len(frames) == 0:
        raise RuntimeError("æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ•°æ®æ–‡ä»¶ï¼è¯·æ£€æŸ¥æ•°æ®ç›®å½•å’Œæ–‡ä»¶æ ¼å¼ã€‚")

    if show_progress:
        print(f"æˆåŠŸè¯»å– {len(frames)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹åˆå¹¶æ•°æ®æ¡†...")
    # ä½¿ç”¨sort=FalseåŠ é€Ÿconcat
    concatenated = pd.concat(frames, ignore_index=True, sort=False)
    if show_progress:
        print(f"æ•°æ®åˆå¹¶å®Œæˆï¼Œå…± {len(concatenated):,} æ¡è®°å½•")
    return concatenated, log_paths


def compute_user_item_matrix(
    df: pd.DataFrame,
    sample_users: Optional[int] = None,
    min_interactions: int = 5,
    show_progress: bool = False,
) -> Tuple[csr_matrix, np.ndarray, np.ndarray]:
    """
    æ„å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µï¼ˆç¨€ç–çŸ©é˜µï¼‰ã€‚
    
    Returns:
        user_item_matrix: ç¨€ç–çŸ©é˜µï¼Œshape=(n_users, n_items)
        user_ids: ç”¨æˆ·IDæ•°ç»„
        item_ids: ç‰©å“IDæ•°ç»„
    """
    required_cols = {"user_id", "video_id"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {sorted(missing)}")

    # è¿‡æ»¤äº¤äº’æ¬¡æ•°å¤ªå°‘çš„ç”¨æˆ·
    user_counts = df.groupby("user_id").size()
    valid_users = user_counts[user_counts >= min_interactions].index
    df_filtered = df[df["user_id"].isin(valid_users)].copy()

    if show_progress:
        print(f"è¿‡æ»¤åç”¨æˆ·æ•°: {len(valid_users):,} (åŸå§‹: {df['user_id'].nunique():,})")

    # é‡‡æ ·ç”¨æˆ·ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if sample_users is not None and sample_users < len(valid_users):
        sampled_user_ids = np.random.choice(
            valid_users, size=sample_users, replace=False
        )
        df_filtered = df_filtered[df_filtered["user_id"].isin(sampled_user_ids)]
        valid_users = sampled_user_ids
        if show_progress:
            print(f"é‡‡æ ·ç”¨æˆ·æ•°: {len(valid_users):,}")

    if show_progress:
        print("æ„å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µ...")
    
    # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹æ³•è·å–å”¯ä¸€ç‰©å“
    unique_items = np.sort(df_filtered["video_id"].unique())
    item_to_idx = {item: idx for idx, item in enumerate(unique_items)}
    user_to_idx = {user: idx for idx, user in enumerate(valid_users)}
    
    # ä¼˜åŒ–ï¼šä½¿ç”¨groupbyè·å–æ¯ä¸ªç”¨æˆ·çš„å”¯ä¸€ç‰©å“ï¼ˆä½¿ç”¨numpy uniqueæ›´å¿«ï¼‰
    user_items_dict = df_filtered.groupby("user_id", sort=False)["video_id"].apply(
        lambda x: np.unique(x.values)
    ).to_dict()

    # æ„å»ºç¨€ç–çŸ©é˜µçš„ç´¢å¼•
    rows = []
    cols = []
    
    # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼å¯èƒ½æ›´å¿«ï¼Œä½†è¿™é‡Œä¿æŒå¾ªç¯ä»¥ä¾¿è°ƒè¯•
    for user_id, items in user_items_dict.items():
        user_idx = user_to_idx[user_id]
        for item_id in items:
            item_idx = item_to_idx[item_id]
            rows.append(user_idx)
            cols.append(item_idx)

    # ä½¿ç”¨float32èŠ‚çœå†…å­˜
    data = np.ones(len(rows), dtype=np.float32)
    user_item_matrix = csr_matrix(
        (data, (rows, cols)), 
        shape=(len(valid_users), len(unique_items)),
        dtype=np.float32
    )

    user_ids = np.array(valid_users, dtype=np.int32)
    item_ids = np.array(unique_items, dtype=np.int32)

    return user_item_matrix, user_ids, item_ids


def compute_jaccard_similarity(
    user_item_matrix: csr_matrix, show_progress: bool = False
) -> np.ndarray:
    """
    è®¡ç®—ç”¨æˆ·ä¹‹é—´çš„Jaccardç›¸ä¼¼åº¦çŸ©é˜µã€‚
    
    Jaccardç›¸ä¼¼åº¦ = |A âˆ© B| / |A âˆª B|
    """
    n_users = user_item_matrix.shape[0]

    if show_progress:
        print(f"è®¡ç®— {n_users} ä¸ªç”¨æˆ·ä¹‹é—´çš„Jaccardç›¸ä¼¼åº¦...")

    # ä¼˜åŒ–ï¼šä½¿ç”¨float32å‡å°‘å†…å­˜ä½¿ç”¨
    # è®¡ç®—äº¤é›†ï¼šçŸ©é˜µä¹˜æ³•å¾—åˆ°äº¤é›†å¤§å°
    intersection = user_item_matrix.dot(user_item_matrix.T)
    intersection = intersection.toarray().astype(np.float32)

    # è®¡ç®—å¹¶é›†ï¼š|A âˆª B| = |A| + |B| - |A âˆ© B|
    user_sizes = np.array(user_item_matrix.sum(axis=1), dtype=np.float32).flatten()
    union = user_sizes[:, None] + user_sizes[None, :] - intersection

    # é¿å…é™¤ä»¥é›¶
    union = np.maximum(union, 1e-10)
    jaccard = (intersection / union).astype(np.float32)

    # å°†å¯¹è§’çº¿è®¾ä¸º1ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„ç›¸ä¼¼åº¦ï¼‰
    np.fill_diagonal(jaccard, 1.0)

    return jaccard


def compute_cosine_similarity(
    user_item_matrix: csr_matrix, show_progress: bool = False
) -> np.ndarray:
    """è®¡ç®—ç”¨æˆ·ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µã€‚"""
    n_users = user_item_matrix.shape[0]

    if show_progress:
        print(f"è®¡ç®— {n_users} ä¸ªç”¨æˆ·ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦...")

    # ä¼˜åŒ–ï¼šä½¿ç”¨float32
    # L2å½’ä¸€åŒ–
    norms = np.sqrt(np.array(user_item_matrix.power(2).sum(axis=1), dtype=np.float32)).flatten()
    norms = np.maximum(norms, 1e-10)
    normalized_matrix = user_item_matrix.multiply(1.0 / norms[:, None])

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    cosine = normalized_matrix.dot(normalized_matrix.T).toarray().astype(np.float32)

    return cosine


def perform_kmeans_clustering(
    user_item_matrix: csr_matrix,
    similarity_matrix: np.ndarray,
    n_clusters: int,
    random_state: int = 42,
    compute_silhouette: bool = True,
    silhouette_sample_size: Optional[int] = None,
) -> Tuple[np.ndarray, Optional[float]]:
    """
    ä½¿ç”¨K-meanså¯¹ç”¨æˆ·è¿›è¡Œèšç±»ã€‚
    
    Args:
        compute_silhouette: æ˜¯å¦è®¡ç®—è½®å»“ç³»æ•°ï¼ˆå¤§è§„æ¨¡æ•°æ®å¯èƒ½å¾ˆæ…¢ï¼‰
        silhouette_sample_size: è®¡ç®—è½®å»“ç³»æ•°æ—¶çš„é‡‡æ ·å¤§å°ï¼ˆNoneè¡¨ç¤ºä¸é‡‡æ ·ï¼‰
    
    Returns:
        labels: èšç±»æ ‡ç­¾
        silhouette: è½®å»“ç³»æ•°ï¼ˆå¦‚æœcompute_silhouette=Falseåˆ™ä¸ºNoneï¼‰
    """
    n_users = user_item_matrix.shape[0]
    
    # ä¼˜åŒ–ï¼šå¯¹äºå¤§è§„æ¨¡æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨TruncatedSVDé™ç»´ï¼ˆæ¯”PCAå¿«ï¼‰
    if n_users > 1000:
        n_components = min(50, n_users - 1, user_item_matrix.shape[1])
        if n_users > 10000:
            n_components = min(30, n_users - 1, user_item_matrix.shape[1])
        
        # ä½¿ç”¨TruncatedSVDï¼ˆé€‚åˆç¨€ç–çŸ©é˜µï¼Œæ¯”PCAå¿«ï¼‰
        svd = TruncatedSVD(
            n_components=n_components, 
            random_state=random_state,
            n_iter=5,  # å‡å°‘è¿­ä»£æ¬¡æ•°ä»¥åŠ é€Ÿ
            algorithm='arpack'  # å¯¹äºç¨€ç–çŸ©é˜µæ›´å¿«
        )
        features = svd.fit_transform(user_item_matrix).astype(np.float32)
    else:
        features = similarity_matrix.astype(np.float32)

    # ä¼˜åŒ–ï¼šå‡å°‘n_initæ¬¡æ•°ï¼ˆå¤§è§„æ¨¡æ•°æ®æ—¶3-5æ¬¡é€šå¸¸è¶³å¤Ÿï¼‰
    n_init = 3 if n_users > 5000 else 5
    
    kmeans = KMeans(
        n_clusters=n_clusters, 
        random_state=random_state, 
        n_init=n_init,
        max_iter=300,  # å‡å°‘æœ€å¤§è¿­ä»£æ¬¡æ•°
        algorithm='lloyd',  # æ˜ç¡®æŒ‡å®šç®—æ³•
    )
    labels = kmeans.fit_predict(features)

    # è®¡ç®—è½®å»“ç³»æ•°ï¼ˆå¯é€‰ï¼Œå¤§è§„æ¨¡æ•°æ®å¯èƒ½å¾ˆæ…¢ï¼‰
    silhouette = None
    if compute_silhouette:
        # å¯¹äºå¤§è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨é‡‡æ ·æ–¹æ³•è®¡ç®—è½®å»“ç³»æ•°
        if silhouette_sample_size is not None and silhouette_sample_size < n_users:
            # é‡‡æ ·ç”¨æˆ·è®¡ç®—è½®å»“ç³»æ•°
            sample_indices = np.random.choice(
                n_users, size=silhouette_sample_size, replace=False
            )
            sample_labels = labels[sample_indices]
            sample_distance = 1 - similarity_matrix[np.ix_(sample_indices, sample_indices)]
            np.fill_diagonal(sample_distance, 0)
            silhouette = silhouette_score(
                sample_distance, sample_labels, metric="precomputed"
            )
        else:
            # è®¡ç®—å®Œæ•´è½®å»“ç³»æ•°
            distance_matrix = 1 - similarity_matrix
            np.fill_diagonal(distance_matrix, 0)
            silhouette = silhouette_score(distance_matrix, labels, metric="precomputed")

    return labels, silhouette


def perform_hierarchical_clustering(
    similarity_matrix: np.ndarray,
    n_clusters: int,
    method: str = "complete",
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ä½¿ç”¨å±‚æ¬¡èšç±»å¯¹ç”¨æˆ·è¿›è¡Œèšç±»ã€‚
    
    Args:
        similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ
        n_clusters: èšç±»æ•°é‡
        method: é“¾æ¥æ–¹æ³• ('complete', 'average', 'ward')
        æ³¨æ„: 'ward' æ–¹æ³•éœ€è¦æ¬§æ°è·ç¦»ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨å¹³æ–¹æ¬§æ°è·ç¦»
    """
    # å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºè·ç¦»
    distance_matrix = 1 - similarity_matrix
    np.fill_diagonal(distance_matrix, 0)

    # wardæ–¹æ³•éœ€è¦æ¬§æ°è·ç¦»ï¼Œä½¿ç”¨å¹³æ–¹æ¬§æ°è·ç¦»
    if method == "ward":
        # å°†è·ç¦»çŸ©é˜µè½¬æ¢ä¸ºå¹³æ–¹æ¬§æ°è·ç¦»å½¢å¼
        # ä½¿ç”¨ condensed distance matrix
        from scipy.spatial.distance import squareform
        condensed_dist = squareform(distance_matrix, checks=False)
        linkage_matrix = linkage(condensed_dist, method=method)
    else:
        # å¯¹äºå…¶ä»–æ–¹æ³•ï¼Œä½¿ç”¨condensed distance matrix
        from scipy.spatial.distance import squareform
        condensed_dist = squareform(distance_matrix, checks=False)
        linkage_matrix = linkage(condensed_dist, method=method)

    # è·å–èšç±»æ ‡ç­¾
    labels = fcluster(linkage_matrix, n_clusters, criterion="maxclust") - 1

    return labels, linkage_matrix


def analyze_cluster_statistics(
    user_ids: np.ndarray,
    user_item_matrix: csr_matrix,
    labels: np.ndarray,
    similarity_matrix: np.ndarray,
) -> Dict:
    """åˆ†æèšç±»ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    n_clusters = len(np.unique(labels))
    cluster_stats = []

    for cluster_id in range(n_clusters):
        mask = labels == cluster_id
        cluster_users = user_ids[mask]
        cluster_size = len(cluster_users)

        # è®¡ç®—ç°‡å†…å¹³å‡ç›¸ä¼¼åº¦
        cluster_sim = similarity_matrix[mask][:, mask]
        intra_cluster_sim = cluster_sim[np.triu_indices(cluster_size, k=1)].mean()

        # è®¡ç®—ç°‡é—´å¹³å‡ç›¸ä¼¼åº¦
        inter_cluster_sims = []
        for other_cluster_id in range(n_clusters):
            if other_cluster_id != cluster_id:
                other_mask = labels == other_cluster_id
                inter_sim = similarity_matrix[mask][:, other_mask].mean()
                inter_cluster_sims.append(inter_sim)
        inter_cluster_sim = np.mean(inter_cluster_sims) if inter_cluster_sims else 0.0

        # è®¡ç®—ç°‡å†…ç”¨æˆ·è®¿é—®çš„ç‰©å“é›†åˆ
        cluster_items = user_item_matrix[mask].sum(axis=0).A1 > 0
        cluster_unique_items = cluster_items.sum()

        cluster_stats.append({
            "cluster_id": int(cluster_id),
            "size": int(cluster_size),
            "intra_cluster_similarity": float(intra_cluster_sim),
            "inter_cluster_similarity": float(inter_cluster_sim),
            "unique_items": int(cluster_unique_items),
            "user_ids": cluster_users.tolist(),
        })

    # è®¡ç®—æ•´ä½“ç»Ÿè®¡
    overall_intra_sim = np.mean([
        stat["intra_cluster_similarity"] for stat in cluster_stats
    ])
    overall_inter_sim = np.mean([
        stat["inter_cluster_similarity"] for stat in cluster_stats
    ])

    return {
        "n_clusters": n_clusters,
        "total_users": len(user_ids),
        "overall_intra_cluster_similarity": float(overall_intra_sim),
        "overall_inter_cluster_similarity": float(overall_inter_sim),
        "clustering_quality": float(overall_intra_sim - overall_inter_sim),
        "cluster_statistics": cluster_stats,
    }


def plot_similarity_heatmap(
    similarity_matrix: np.ndarray,
    output_path: Path,
    labels: Optional[np.ndarray] = None,
    max_users: int = 500,
) -> None:
    """ç»˜åˆ¶ç›¸ä¼¼åº¦çŸ©é˜µçƒ­åŠ›å›¾ã€‚"""
    if plt is None or sns is None:
        print("matplotlib/seabornæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    n_users = similarity_matrix.shape[0]

    # å¦‚æœç”¨æˆ·å¤ªå¤šï¼Œé‡‡æ ·æ˜¾ç¤º
    if n_users > max_users:
        indices = np.random.choice(n_users, size=max_users, replace=False)
        sim_subset = similarity_matrix[np.ix_(indices, indices)]
        labels_subset = labels[indices] if labels is not None else None
    else:
        sim_subset = similarity_matrix
        labels_subset = labels
        indices = np.arange(n_users)

    # å¦‚æœæœ‰æ ‡ç­¾ï¼ŒæŒ‰èšç±»æ’åº
    if labels_subset is not None:
        sort_order = np.argsort(labels_subset)
        sim_subset = sim_subset[np.ix_(sort_order, sort_order)]
        labels_subset = labels_subset[sort_order]

    fig, ax = plt.subplots(figsize=(12, 10))
    sns.heatmap(
        sim_subset,
        cmap="YlOrRd",
        square=True,
        cbar_kws={"label": "ç›¸ä¼¼åº¦"},
        ax=ax,
        vmin=0,
        vmax=1,
    )
    ax.set_title(f"ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µçƒ­åŠ›å›¾ (é‡‡æ · {len(sim_subset)} ä¸ªç”¨æˆ·)", fontsize=14)
    ax.set_xlabel("ç”¨æˆ·ç´¢å¼•")
    ax.set_ylabel("ç”¨æˆ·ç´¢å¼•")

    if labels_subset is not None:
        # æ·»åŠ èšç±»è¾¹ç•Œ
        unique_labels = np.unique(labels_subset)
        for label in unique_labels:
            mask = labels_subset == label
            boundary = np.where(mask)[0]
            if len(boundary) > 0:
                start = boundary[0]
                end = boundary[-1] + 1
                ax.axhline(start, color="blue", linewidth=2, alpha=0.7)
                ax.axhline(end, color="blue", linewidth=2, alpha=0.7)
                ax.axvline(start, color="blue", linewidth=2, alpha=0.7)
                ax.axvline(end, color="blue", linewidth=2, alpha=0.7)

    fig.tight_layout()
    fig.savefig(output_path, dpi=200, bbox_inches="tight")
    plt.close(fig)


def plot_cluster_distribution(
    labels: np.ndarray,
    similarity_matrix: np.ndarray,
    output_path: Path,
) -> None:
    """ç»˜åˆ¶èšç±»åˆ†å¸ƒå’Œç›¸ä¼¼åº¦åˆ†å¸ƒã€‚"""
    if plt is None:
        print("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # èšç±»å¤§å°åˆ†å¸ƒ
    unique_labels, counts = np.unique(labels, return_counts=True)
    axes[0].bar(unique_labels, counts, alpha=0.7, color="steelblue")
    axes[0].set_xlabel("èšç±»ID")
    axes[0].set_ylabel("ç”¨æˆ·æ•°é‡")
    axes[0].set_title("å„èšç±»çš„ç”¨æˆ·æ•°é‡åˆ†å¸ƒ")
    axes[0].grid(True, alpha=0.3)

    # ç°‡å†…å’Œç°‡é—´ç›¸ä¼¼åº¦åˆ†å¸ƒ
    n_clusters = len(unique_labels)
    intra_sims = []
    inter_sims = []

    for cluster_id in unique_labels:
        mask = labels == cluster_id
        cluster_sim = similarity_matrix[mask][:, mask]
        intra_sims.extend(
            cluster_sim[np.triu_indices(np.sum(mask), k=1)].tolist()
        )

        for other_cluster_id in unique_labels:
            if other_cluster_id != cluster_id:
                other_mask = labels == other_cluster_id
                inter_sim = similarity_matrix[mask][:, other_mask]
                inter_sims.extend(inter_sim.flatten().tolist())

    axes[1].hist(
        intra_sims,
        bins=50,
        alpha=0.6,
        label=f"ç°‡å†…ç›¸ä¼¼åº¦ (å‡å€¼={np.mean(intra_sims):.3f})",
        color="green",
    )
    axes[1].hist(
        inter_sims,
        bins=50,
        alpha=0.6,
        label=f"ç°‡é—´ç›¸ä¼¼åº¦ (å‡å€¼={np.mean(inter_sims):.3f})",
        color="red",
    )
    axes[1].set_xlabel("ç›¸ä¼¼åº¦")
    axes[1].set_ylabel("é¢‘æ•°")
    axes[1].set_title("ç°‡å†… vs ç°‡é—´ç›¸ä¼¼åº¦åˆ†å¸ƒ")
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    fig.tight_layout()
    fig.savefig(output_path, dpi=200, bbox_inches="tight")
    plt.close(fig)


def plot_dendrogram(
    linkage_matrix: np.ndarray,
    output_path: Path,
    max_display: int = 100,
) -> None:
    """ç»˜åˆ¶å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾ã€‚"""
    if plt is None:
        print("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    fig, ax = plt.subplots(figsize=(15, 8))
    dendrogram(
        linkage_matrix,
        truncate_mode="lastp",
        p=max_display,
        leaf_rotation=90,
        leaf_font_size=8,
        ax=ax,
    )
    ax.set_title("ç”¨æˆ·å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾", fontsize=14)
    ax.set_xlabel("ç”¨æˆ·ç´¢å¼•")
    ax.set_ylabel("è·ç¦»")
    fig.tight_layout()
    fig.savefig(output_path, dpi=200, bbox_inches="tight")
    plt.close(fig)


def main() -> None:
    parser = argparse.ArgumentParser(description="åˆ†æç”¨æˆ·èšç±»æ•ˆåº”")
    parser.add_argument(
        "--data-dir",
        type=Path,
        help="åŒ…å«KuaiRandæ—¥å¿—CSVæ–‡ä»¶çš„ç›®å½•ã€‚",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("reports"),
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: reportsï¼‰ã€‚",
    )
    parser.add_argument(
        "--sample-users",
        type=int,
        default=None,
        help="é‡‡æ ·ç”¨æˆ·æ•°é‡ï¼ˆç”¨äºåŠ é€Ÿè®¡ç®—ï¼Œé»˜è®¤ä¸é‡‡æ ·ï¼‰ã€‚",
    )
    parser.add_argument(
        "--min-interactions",
        type=int,
        default=5,
        help="ç”¨æˆ·æœ€å°‘äº¤äº’æ¬¡æ•°ï¼ˆé»˜è®¤: 5ï¼‰ã€‚",
    )
    parser.add_argument(
        "--n-clusters",
        type=int,
        default=5,
        help="èšç±»æ•°é‡ï¼ˆé»˜è®¤: 5ï¼‰ã€‚",
    )
    parser.add_argument(
        "--clustering-method",
        type=str,
        choices=["kmeans", "hierarchical"],
        default="kmeans",
        help="èšç±»æ–¹æ³•ï¼ˆé»˜è®¤: kmeansï¼‰ã€‚",
    )
    parser.add_argument(
        "--similarity-metric",
        type=str,
        choices=["jaccard", "cosine"],
        default="jaccard",
        help="ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ï¼ˆé»˜è®¤: jaccardï¼‰ã€‚",
    )
    parser.add_argument(
        "--show-progress",
        action="store_true",
        help="æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯ã€‚",
    )
    parser.add_argument(
        "--max-heatmap-users",
        type=int,
        default=500,
        help="çƒ­åŠ›å›¾æœ€å¤§æ˜¾ç¤ºç”¨æˆ·æ•°ï¼ˆé»˜è®¤: 500ï¼‰ã€‚",
    )
    parser.add_argument(
        "--skip-silhouette",
        action="store_true",
        help="è·³è¿‡è½®å»“ç³»æ•°è®¡ç®—ï¼ˆå¤§è§„æ¨¡æ•°æ®æ—¶æ¨èä½¿ç”¨ï¼Œå¯æ˜¾è‘—åŠ é€Ÿï¼‰ã€‚",
    )
    parser.add_argument(
        "--silhouette-sample-size",
        type=int,
        default=None,
        help="è®¡ç®—è½®å»“ç³»æ•°æ—¶çš„é‡‡æ ·å¤§å°ï¼ˆé»˜è®¤ä¸é‡‡æ ·ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰ã€‚",
    )
    parser.add_argument(
        "--n-workers",
        type=int,
        default=None,
        help="å¹¶è¡Œè¯»å–CSVæ–‡ä»¶çš„è¿›ç¨‹æ•°ï¼ˆé»˜è®¤ï¼šè‡ªåŠ¨é€‰æ‹©ï¼Œæœ€å¤š8ä¸ªï¼‰ã€‚",
    )

    args = parser.parse_args()

    # ç¡®å®šæ•°æ®ç›®å½•
    default_data_dir = (
        Path(__file__).resolve().parents[1] / "data" / "KuaiRand-1K" / "data"
    )
    data_dir = (args.data_dir or default_data_dir).expanduser()
    if not data_dir.exists():
        raise FileNotFoundError(
            f"æ•°æ®ç›®å½• '{data_dir}' ä¸å­˜åœ¨ã€‚"
            "è¯·ä½¿ç”¨ --data-dir æŒ‡å®šåŒ…å«KuaiRand CSVæ–‡ä»¶çš„ç›®å½•ã€‚"
        )

    # åˆ›å»ºè¾“å‡ºç›®å½•
    args.output_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½æ•°æ®
    if args.show_progress:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] å¼€å§‹åŠ è½½æ•°æ®...")
    load_start = time.time()
    n_workers = getattr(args, 'n_workers', None)
    df, log_paths = _load_log_frames(data_dir, args.show_progress, n_workers=n_workers)
    load_time = time.time() - load_start
    if args.show_progress:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] æ•°æ®åŠ è½½å®Œæˆ: {len(df):,} æ¡äº¤äº’è®°å½•ï¼Œæ¥è‡ª {len(log_paths)} ä¸ªæ—¥å¿—æ–‡ä»¶ (è€—æ—¶: {load_time:.1f}ç§’)")

    # æ„å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µ
    if args.show_progress:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] å¼€å§‹æ„å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µ...")
    matrix_start = time.time()
    user_item_matrix, user_ids, item_ids = compute_user_item_matrix(
        df,
        sample_users=args.sample_users,
        min_interactions=args.min_interactions,
        show_progress=args.show_progress,
    )
    matrix_time = time.time() - matrix_start

    n_users = user_item_matrix.shape[0]
    n_items = user_item_matrix.shape[1]
    
    if args.show_progress:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ç”¨æˆ·-ç‰©å“çŸ©é˜µæ„å»ºå®Œæˆ: {n_users:,} ç”¨æˆ· Ã— {n_items:,} ç‰©å“ (è€—æ—¶: {matrix_time:.1f}ç§’)")
        
        # å†…å­˜ä½¿ç”¨ä¼°ç®—å’Œè­¦å‘Š
        estimated_memory_gb = (n_users * n_users * 8) / (1024**3)  # float64
        print(
            f"é¢„è®¡ç›¸ä¼¼åº¦çŸ©é˜µå†…å­˜ä½¿ç”¨: ~{estimated_memory_gb:.2f} GB"
        )
        if estimated_memory_gb > 5:
            print(
                f"âš ï¸  è­¦å‘Š: ç›¸ä¼¼åº¦çŸ©é˜µè¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦è¾ƒå¤šå†…å­˜å’Œæ—¶é—´ã€‚"
            )
            if not args.skip_silhouette:
                print(
                    f"   å»ºè®®ä½¿ç”¨ --skip-silhouette è·³è¿‡è½®å»“ç³»æ•°è®¡ç®—ä»¥åŠ é€Ÿã€‚"
                )

    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    if args.show_progress:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] å¼€å§‹è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ (æ–¹æ³•: {args.similarity_metric})...")
    similarity_start = time.time()
    if args.similarity_metric == "jaccard":
        similarity_matrix = compute_jaccard_similarity(
            user_item_matrix, show_progress=args.show_progress
        )
    else:
        similarity_matrix = compute_cosine_similarity(
            user_item_matrix, show_progress=args.show_progress
        )
    similarity_time = time.time() - similarity_start
    if args.show_progress:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ç›¸ä¼¼åº¦çŸ©é˜µè®¡ç®—å®Œæˆ (è€—æ—¶: {similarity_time:.1f}ç§’, {similarity_time/60:.1f}åˆ†é’Ÿ)")

    if args.show_progress:
        # è®¡ç®—ç”¨æˆ·å¹³å‡è®¿é—®çš„ç‰©å“æ•°é‡
        user_item_counts = np.array(user_item_matrix.sum(axis=1)).flatten()
        avg_items_per_user = user_item_counts.mean()
        
        # æ’é™¤å¯¹è§’çº¿åçš„ç›¸ä¼¼åº¦ç»Ÿè®¡ï¼ˆå¯¹è§’çº¿éƒ½æ˜¯1ï¼Œä¼šå½±å“ç»Ÿè®¡ï¼‰
        mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
        off_diagonal_sim = similarity_matrix[mask]
        
        # è®¡ç®—åˆ†ä½æ•°
        percentiles = [25, 50, 75, 90, 95, 99]
        percentile_values = np.percentile(off_diagonal_sim, percentiles)
        
        # ç»Ÿè®¡é«˜ç›¸ä¼¼åº¦ç”¨æˆ·å¯¹çš„æ•°é‡
        high_sim_thresholds = [0.1, 0.2, 0.3, 0.5]
        high_sim_counts = {
            threshold: (off_diagonal_sim >= threshold).sum()
            for threshold in high_sim_thresholds
        }
        
        print(
            f"\nç›¸ä¼¼åº¦çŸ©é˜µç»Ÿè®¡:"
        )
        print(
            f"  ç”¨æˆ·å¹³å‡è®¿é—®ç‰©å“æ•°: {avg_items_per_user:.1f} "
            f"(èŒƒå›´: {user_item_counts.min():.0f} - {user_item_counts.max():.0f})"
        )
        print(
            f"  æ€»ç‰©å“æ•°: {user_item_matrix.shape[1]:,}"
        )
        print(
            f"  å®Œæ•´çŸ©é˜µç»Ÿè®¡ (åŒ…å«å¯¹è§’çº¿): "
            f"å‡å€¼={similarity_matrix.mean():.4f}, "
            f"ä¸­ä½æ•°={np.median(similarity_matrix):.4f}"
        )
        print(
            f"  éå¯¹è§’çº¿ç›¸ä¼¼åº¦ç»Ÿè®¡ (æ’é™¤è‡ªå·±): "
            f"å‡å€¼={off_diagonal_sim.mean():.4f}, "
            f"ä¸­ä½æ•°={np.median(off_diagonal_sim):.4f}, "
            f"æœ€å¤§å€¼={off_diagonal_sim.max():.4f}"
        )
        print(
            f"  ç›¸ä¼¼åº¦åˆ†ä½æ•°: "
            + ", ".join([
                f"P{p}={v:.4f}" 
                for p, v in zip(percentiles, percentile_values)
            ])
        )
        print(
            f"  é«˜ç›¸ä¼¼åº¦ç”¨æˆ·å¯¹æ•°é‡: "
            + ", ".join([
                f"â‰¥{t:.1f}: {cnt:,} ({cnt*100/len(off_diagonal_sim):.2f}%)"
                for t, cnt in high_sim_counts.items()
            ])
        )
        
        # åˆ¤æ–­æ˜¯å¦å­˜åœ¨èšç±»æ•ˆåº”
        if off_diagonal_sim.mean() < 0.01:
            print(
                f"\nâš ï¸  è­¦å‘Š: ç”¨æˆ·é—´å¹³å‡ç›¸ä¼¼åº¦å¾ˆä½ ({off_diagonal_sim.mean():.4f})ï¼Œ"
                f"å¯èƒ½å­˜åœ¨ä»¥ä¸‹æƒ…å†µï¼š"
            )
            print(
                f"  1. ç‰©å“ç©ºé—´å¾ˆå¤§ï¼Œç”¨æˆ·è®¿é—®çš„ç‰©å“é›†åˆé‡å å¾ˆå°"
            )
            print(
                f"  2. ç”¨æˆ·åå¥½å·®å¼‚å¾ˆå¤§ï¼Œä¸å­˜åœ¨æ˜æ˜¾çš„èšç±»æ•ˆåº”"
            )
            print(
                f"  3. å¯èƒ½éœ€è¦å¢åŠ é‡‡æ ·ç”¨æˆ·æ•°æˆ–è°ƒæ•´min-interactionså‚æ•°"
            )
        elif off_diagonal_sim.mean() > 0.1:
            print(
                f"\nâœ“ ç”¨æˆ·é—´å¹³å‡ç›¸ä¼¼åº¦è¾ƒé«˜ ({off_diagonal_sim.mean():.4f})ï¼Œ"
                f"å¯èƒ½å­˜åœ¨èšç±»æ•ˆåº”"
            )

    # éªŒè¯clusteræ•°é‡çš„åˆç†æ€§
    n_users = user_item_matrix.shape[0]
    if args.n_clusters > n_users:
        raise ValueError(
            f"é”™è¯¯: clusteræ•°é‡ ({args.n_clusters}) ä¸èƒ½å¤§äºç”¨æˆ·æ•°é‡ ({n_users})"
        )
    if args.n_clusters > n_users / 2:
        print(
            f"\nâš ï¸  è­¦å‘Š: clusteræ•°é‡ ({args.n_clusters}) ç›¸å¯¹äºç”¨æˆ·æ•°é‡ ({n_users}) è¾ƒå¤šï¼Œ"
            f"å¹³å‡æ¯ä¸ªclusteråªæœ‰ {n_users/args.n_clusters:.1f} ä¸ªç”¨æˆ·ã€‚"
        )
        print(
            f"   è¿™å¯èƒ½å¯¼è‡´è¿‡åº¦åˆ†å‰²ï¼Œå»ºè®®clusteræ•°é‡ä¸è¶…è¿‡ç”¨æˆ·æ•°é‡çš„1/10 ({n_users//10})ã€‚"
        )
    elif args.n_clusters < n_users / 100:
        print(
            f"\nğŸ’¡ æç¤º: clusteræ•°é‡ ({args.n_clusters}) ç›¸å¯¹è¾ƒå°‘ï¼Œ"
            f"å¹³å‡æ¯ä¸ªclusteræœ‰ {n_users/args.n_clusters:.1f} ä¸ªç”¨æˆ·ã€‚"
        )
        print(
            f"   å¦‚æœç”¨æˆ·åå¥½å·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦æ›´å¤šclusteræ¥åŒºåˆ†ä¸åŒçš„ç”¨æˆ·ç¾¤ä½“ã€‚"
        )

    # æ‰§è¡Œèšç±»
    if args.show_progress:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] å¼€å§‹æ‰§è¡Œèšç±» (æ–¹æ³•: {args.clustering_method}, n_clusters: {args.n_clusters})...")
    clustering_start = time.time()
    if args.clustering_method == "kmeans":
        labels, silhouette = perform_kmeans_clustering(
            user_item_matrix,
            similarity_matrix,
            args.n_clusters,
            compute_silhouette=not args.skip_silhouette,
            silhouette_sample_size=args.silhouette_sample_size,
        )
        linkage_matrix = None
        clustering_time = time.time() - clustering_start
        if args.show_progress:
            if silhouette is not None:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] K-meansèšç±»å®Œæˆï¼Œè½®å»“ç³»æ•°: {silhouette:.4f} (è€—æ—¶: {clustering_time:.1f}ç§’, {clustering_time/60:.1f}åˆ†é’Ÿ)")
            else:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] K-meansèšç±»å®Œæˆï¼ˆå·²è·³è¿‡è½®å»“ç³»æ•°è®¡ç®—ï¼‰(è€—æ—¶: {clustering_time:.1f}ç§’, {clustering_time/60:.1f}åˆ†é’Ÿ)")
    else:
        labels, linkage_matrix = perform_hierarchical_clustering(
            similarity_matrix,
            args.n_clusters,
        )
        # è®¡ç®—è½®å»“ç³»æ•°ï¼ˆå¯é€‰ï¼‰
        silhouette = None
        if not args.skip_silhouette:
            if args.silhouette_sample_size is not None and args.silhouette_sample_size < n_users:
                # é‡‡æ ·è®¡ç®—
                sample_indices = np.random.choice(
                    n_users, size=args.silhouette_sample_size, replace=False
                )
                sample_labels = labels[sample_indices]
                sample_distance = 1 - similarity_matrix[np.ix_(sample_indices, sample_indices)]
                np.fill_diagonal(sample_distance, 0)
                silhouette = silhouette_score(
                    sample_distance, sample_labels, metric="precomputed"
                )
            else:
                # å®Œæ•´è®¡ç®—
                distance_matrix = 1 - similarity_matrix
                np.fill_diagonal(distance_matrix, 0)
                silhouette = silhouette_score(
                    distance_matrix, labels, metric="precomputed"
                )
        if args.show_progress:
            if silhouette is not None:
                print(f"å±‚æ¬¡èšç±»å®Œæˆï¼Œè½®å»“ç³»æ•°: {silhouette:.4f}")
            else:
                print(f"å±‚æ¬¡èšç±»å®Œæˆï¼ˆå·²è·³è¿‡è½®å»“ç³»æ•°è®¡ç®—ï¼‰")

    # åˆ†æèšç±»ç»Ÿè®¡
    if args.show_progress:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] å¼€å§‹åˆ†æèšç±»ç»Ÿè®¡...")
    stats_start = time.time()
    cluster_stats = analyze_cluster_statistics(
        user_ids, user_item_matrix, labels, similarity_matrix
    )
    stats_time = time.time() - stats_start
    if args.show_progress:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] èšç±»ç»Ÿè®¡åˆ†æå®Œæˆ (è€—æ—¶: {stats_time:.1f}ç§’)")

    if args.show_progress:
        # è®¡ç®—clusterå¤§å°ç»Ÿè®¡
        cluster_sizes = [stat["size"] for stat in cluster_stats["cluster_statistics"]]
        cluster_sizes_array = np.array(cluster_sizes)
        
        print(f"\nèšç±»åˆ†æç»“æœ:")
        print(f"  èšç±»æ•°é‡: {cluster_stats['n_clusters']}")
        print(f"  æ€»ç”¨æˆ·æ•°: {cluster_stats['total_users']}")
        print(f"  å¹³å‡æ¯ä¸ªclusterç”¨æˆ·æ•°: {np.mean(cluster_sizes_array):.1f}")
        print(f"  Clusterå¤§å°ç»Ÿè®¡: "
              f"æœ€å°={cluster_sizes_array.min()}, "
              f"æœ€å¤§={cluster_sizes_array.max()}, "
              f"ä¸­ä½æ•°={np.median(cluster_sizes_array):.1f}, "
              f"æ ‡å‡†å·®={cluster_sizes_array.std():.1f}")
        
        # ç»Ÿè®¡å°clusterï¼ˆå°‘äº5ä¸ªç”¨æˆ·ï¼‰çš„æ•°é‡
        small_clusters = (cluster_sizes_array < 5).sum()
        if small_clusters > 0:
            print(f"  âš ï¸  å°clusteræ•°é‡ï¼ˆ<5ç”¨æˆ·ï¼‰: {small_clusters} ({small_clusters*100/cluster_stats['n_clusters']:.1f}%)")
        
        print(f"  å¹³å‡ç°‡å†…ç›¸ä¼¼åº¦: {cluster_stats['overall_intra_cluster_similarity']:.4f}")
        print(f"  å¹³å‡ç°‡é—´ç›¸ä¼¼åº¦: {cluster_stats['overall_inter_cluster_similarity']:.4f}")
        print(f"  èšç±»è´¨é‡ (ç°‡å†…-ç°‡é—´): {cluster_stats['clustering_quality']:.4f}")
        if silhouette is not None:
            print(f"  è½®å»“ç³»æ•°: {silhouette:.4f}")
        else:
            print(f"  è½®å»“ç³»æ•°: æœªè®¡ç®—ï¼ˆä½¿ç”¨ --skip-silhouette è·³è¿‡ï¼‰")

        # åªæ˜¾ç¤ºå‰20ä¸ªclusterçš„è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœclusterå¤ªå¤šï¼‰
        max_display = 20
        if len(cluster_stats["cluster_statistics"]) > max_display:
            print(f"\nå„èšç±»ç»Ÿè®¡ï¼ˆæ˜¾ç¤ºå‰{max_display}ä¸ªï¼ŒæŒ‰ç”¨æˆ·æ•°æ’åºï¼‰:")
            sorted_stats = sorted(
                cluster_stats["cluster_statistics"],
                key=lambda x: x["size"],
                reverse=True
            )
            for stat in sorted_stats[:max_display]:
                print(
                    f"  èšç±» {stat['cluster_id']}: "
                    f"ç”¨æˆ·æ•°={stat['size']}, "
                    f"ç°‡å†…ç›¸ä¼¼åº¦={stat['intra_cluster_similarity']:.4f}, "
                    f"ç°‡é—´ç›¸ä¼¼åº¦={stat['inter_cluster_similarity']:.4f}, "
                    f"å”¯ä¸€ç‰©å“æ•°={stat['unique_items']}"
                )
            print(f"  ... (è¿˜æœ‰ {len(cluster_stats['cluster_statistics']) - max_display} ä¸ªclusteræœªæ˜¾ç¤º)")
        else:
            print(f"\nå„èšç±»ç»Ÿè®¡:")
            for stat in cluster_stats["cluster_statistics"]:
                print(
                    f"  èšç±» {stat['cluster_id']}: "
                    f"ç”¨æˆ·æ•°={stat['size']}, "
                    f"ç°‡å†…ç›¸ä¼¼åº¦={stat['intra_cluster_similarity']:.4f}, "
                    f"ç°‡é—´ç›¸ä¼¼åº¦={stat['inter_cluster_similarity']:.4f}, "
                    f"å”¯ä¸€ç‰©å“æ•°={stat['unique_items']}"
                )

    # ä¿å­˜ç»“æœ
    results = {
        "similarity_metric": args.similarity_metric,
        "clustering_method": args.clustering_method,
        "n_clusters": args.n_clusters,
        "silhouette_score": float(silhouette) if silhouette is not None else None,
        "cluster_statistics": cluster_stats,
        "user_cluster_mapping": {
            str(uid): int(label) for uid, label in zip(user_ids, labels)
        },
    }

    results_json_path = args.output_dir / "user_clustering_results.json"
    results_json_path.write_text(
        json.dumps(results, indent=2, ensure_ascii=False)
    )
    if args.show_progress:
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_json_path}")

    # ç»˜åˆ¶å¯è§†åŒ–
    if plt is not None:
        # ç›¸ä¼¼åº¦çƒ­åŠ›å›¾
        heatmap_path = args.output_dir / "user_similarity_heatmap.png"
        plot_similarity_heatmap(
            similarity_matrix,
            heatmap_path,
            labels=labels,
            max_users=args.max_heatmap_users,
        )
        if args.show_progress:
            print(f"ç›¸ä¼¼åº¦çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {heatmap_path}")

        # èšç±»åˆ†å¸ƒ
        cluster_dist_path = args.output_dir / "cluster_distribution.png"
        plot_cluster_distribution(labels, similarity_matrix, cluster_dist_path)
        if args.show_progress:
            print(f"èšç±»åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {cluster_dist_path}")

        # å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾ï¼ˆå¦‚æœä½¿ç”¨å±‚æ¬¡èšç±»ï¼‰
        if linkage_matrix is not None:
            dendrogram_path = args.output_dir / "dendrogram.png"
            plot_dendrogram(linkage_matrix, dendrogram_path)
            if args.show_progress:
                print(f"æ ‘çŠ¶å›¾å·²ä¿å­˜åˆ°: {dendrogram_path}")

    # ä¿å­˜ç”¨æˆ·èšç±»æ˜ å°„CSV
    # ä¿å­˜ç”¨æˆ·èšç±»æ˜ å°„
    if args.show_progress:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] å¼€å§‹ä¿å­˜ç»“æœ...")
    save_start = time.time()
    
    user_cluster_df = pd.DataFrame({
        "user_id": user_ids,
        "cluster_id": labels,
    })
    user_cluster_csv_path = args.output_dir / "user_cluster_mapping.csv"
    user_cluster_df.to_csv(user_cluster_csv_path, index=False)
    save_time = time.time() - save_start
    total_time = time.time() - load_start
    
    if args.show_progress:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ç”¨æˆ·èšç±»æ˜ å°„å·²ä¿å­˜åˆ°: {user_cluster_csv_path}")
        print("")
        print("=" * 80)
        print("å®éªŒå®Œæˆ!")
        print("=" * 80)
        print(f"æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ, {total_time/3600:.2f}å°æ—¶)")
        if hasattr(args, 'show_progress') and args.show_progress:
            print(f"  æ•°æ®åŠ è½½: {load_time:.1f}ç§’ ({load_time/total_time*100:.1f}%)")
            print(f"  çŸ©é˜µæ„å»º: {matrix_time:.1f}ç§’ ({matrix_time/total_time*100:.1f}%)")
            print(f"  ç›¸ä¼¼åº¦è®¡ç®—: {similarity_time:.1f}ç§’ ({similarity_time/total_time*100:.1f}%)")
            print(f"  èšç±»æ‰§è¡Œ: {clustering_time:.1f}ç§’ ({clustering_time/total_time*100:.1f}%)")
            print(f"  ç»Ÿè®¡åˆ†æ: {stats_time:.1f}ç§’ ({stats_time/total_time*100:.1f}%)")
            print(f"  ç»“æœä¿å­˜: {save_time:.1f}ç§’ ({save_time/total_time*100:.1f}%)")
        print("=" * 80)


if __name__ == "__main__":
    main()

