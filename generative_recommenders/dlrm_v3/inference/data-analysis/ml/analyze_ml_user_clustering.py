#!/usr/bin/env python3
"""åˆ†æMovieLensæ•°æ®é›†ä¸­ç”¨æˆ·ä¹‹é—´çš„itemé‡åˆåº¦å’Œèšç±»æ•ˆåº”ã€‚"""
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import TruncatedSVD

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    from matplotlib.ticker import FuncFormatter
except Exception:
    plt = None
    sns = None
    FuncFormatter = None

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None


def _find_csvs(data_dir: Path) -> List[Path]:
    """æŸ¥æ‰¾MovieLensæ•°æ®CSVæ–‡ä»¶ã€‚"""
    candidates = []
    # Common SASRec outputs
    for pattern in [
        "sasrec_format.csv",
        "sasrec_format_by_user_train.csv",
        "sasrec_format_by_user_test.csv",
    ]:
        candidates.extend(sorted(data_dir.glob(pattern)))
    # Fallback: any CSV inside the dir
    if not candidates:
        candidates = sorted(data_dir.glob("*.csv"))
    # Deduplicate while preserving order
    seen = set()
    unique: List[Path] = []
    for p in candidates:
        if p not in seen:
            unique.append(p)
            seen.add(p)
    return unique


def _detect_columns(file_path: Path) -> Tuple[Optional[str], Optional[str], Optional[str]]:
    """å¿«é€Ÿæ£€æµ‹CSVæ–‡ä»¶çš„åˆ—åï¼ˆåªè¯»å–ç¬¬ä¸€è¡Œï¼‰ã€‚"""
    try:
        # åªè¯»å–ç¬¬ä¸€è¡Œæ¥æ£€æµ‹åˆ—å
        first_row = pd.read_csv(file_path, nrows=0)
        columns = first_row.columns.tolist()
    except Exception:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•è¯»å–å‰å‡ è¡Œ
        try:
            first_row = pd.read_csv(file_path, nrows=1)
            columns = first_row.columns.tolist()
        except Exception:
            return None, None, None
    
    user_col = None
    item_col = None
    seq_col = None
    
    for c in columns:
        lc = c.lower()
        if lc in {"user", "user_id", "uid"}:
            user_col = c
        if lc in {"item", "item_id", "iid", "video_id", "movie_id"}:
            item_col = c
        if lc in {"sequence_item_ids", "items", "item_ids"}:
            seq_col = c
    
    return user_col, item_col, seq_col


def _load_single_file(p_str: str, show_progress: bool = False) -> pd.DataFrame:
    """åŠ è½½å•ä¸ªCSVæ–‡ä»¶ï¼ˆå†…éƒ¨å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œå¤„ç†ï¼‰ã€‚"""
    p = Path(p_str)
    # å°è¯•ä½¿ç”¨pyarrowå¼•æ“ï¼ˆå¦‚æœå¯ç”¨ï¼Œæ¯”Cå¼•æ“å¿«å¾ˆå¤šï¼‰
    try:
        import pyarrow.csv as pacsv
        HAS_PYARROW = True
    except ImportError:
        HAS_PYARROW = False
    
    # å…ˆæ£€æµ‹åˆ—åï¼ˆå¿«é€Ÿï¼‰
    user_col, item_col, seq_col = _detect_columns(p)
    
    if user_col is None:
        raise ValueError(f"æ–‡ä»¶ {p} ç¼ºå°‘ç”¨æˆ·æ ‡è¯†åˆ—ã€‚")
    
    # å¤„ç†åºåˆ—æ ¼å¼
    if seq_col is not None:
        # åªè¯»å–éœ€è¦çš„åˆ—
        try:
            # å°è¯•ä½¿ç”¨pyarrowï¼ˆæ›´å¿«ï¼‰
            if HAS_PYARROW:
                try:
                    table = pacsv.read_csv(
                        p,
                        convert_options=pacsv.ConvertOptions(
                            include_columns=[user_col, seq_col],
                            column_types={user_col: "int64", seq_col: "string"}
                        )
                    )
                    df = table.to_pandas()
                except Exception:
                    df = pd.read_csv(
                        p,
                        usecols=[user_col, seq_col],
                        dtype={
                            user_col: "int64",
                            seq_col: "object",
                        },
                        engine="c",
                        low_memory=False,
                        na_filter=False,
                    )
            else:
                df = pd.read_csv(
                    p,
                    usecols=[user_col, seq_col],
                    dtype={
                        user_col: "int64",
                        seq_col: "object",  # åºåˆ—æ˜¯å­—ç¬¦ä¸²
                    },
                    engine="c",
                    low_memory=False,
                    na_filter=False,
                )
        except Exception:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•æ ‡å‡†è¯»å–
            df = pd.read_csv(p, usecols=[user_col, seq_col])
        
        tmp = df.rename(
            columns={user_col: "user_id", seq_col: "sequence_item_ids"}
        )
        
        # ä¼˜åŒ–åºåˆ—å±•å¼€ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œ
        split_series = (
            tmp["sequence_item_ids"]
            .astype(str)
            .str.strip()
            .replace({"nan": ""})
            .str.split(r"[,\s]+", regex=True)
        )
        
        # ä½¿ç”¨explodeï¼ˆæ¯”iterrowså¿«å¾—å¤šï¼‰
        tmp["item_id"] = split_series
        exploded = tmp[["user_id", "item_id"]].explode("item_id")
        exploded = exploded[exploded["item_id"] != ""].copy()
        
        try:
            exploded["item_id"] = exploded["item_id"].astype("int64")
        except Exception:
            pass
        
        exploded["__source_file"] = p.name
        return exploded[["user_id", "item_id", "__source_file"]]
    
    # å¤„ç†æ ‡å‡†äº¤äº’æ ¼å¼
    if item_col is None:
        raise ValueError(
            f"æ–‡ä»¶ {p} å¿…é¡»åŒ…å«ç”¨æˆ·å’Œç‰©å“æ ‡è¯†åˆ—ï¼Œæˆ–åŒ…å« 'sequence_item_ids' åˆ—ã€‚"
        )
    
    # åªè¯»å–éœ€è¦çš„åˆ—ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
    # å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨chunksizeåˆ†å—è¯»å–
    file_size_mb = p.stat().st_size / (1024 * 1024)
    use_chunks = file_size_mb > 10  # é™ä½é˜ˆå€¼ï¼Œæ›´æ—©ä½¿ç”¨åˆ†å—è¯»å–
    
    # æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´chunk_sizeï¼ˆå¢å¤§chunk_sizeä»¥æé«˜æ•ˆç‡ï¼‰
    if file_size_mb > 500:
        chunk_size = 2000000  # è¶…å¤§æ–‡ä»¶ï¼šæ¯æ¬¡è¯»å–200ä¸‡è¡Œï¼ˆå¢å¤§ï¼‰
    elif file_size_mb > 100:
        chunk_size = 1000000   # å¤§æ–‡ä»¶ï¼šæ¯æ¬¡è¯»å–100ä¸‡è¡Œï¼ˆå¢å¤§ï¼‰
    else:
        chunk_size = 500000    # ä¸­ç­‰æ–‡ä»¶ï¼šæ¯æ¬¡è¯»å–50ä¸‡è¡Œï¼ˆå¢å¤§ï¼‰
    
    if use_chunks:
        # åˆ†å—è¯»å–å¤§æ–‡ä»¶
        chunks = []
        chunk_iter = pd.read_csv(
            p,
            usecols=[user_col, item_col],
            dtype={
                user_col: "int32",  # ä½¿ç”¨int32èŠ‚çœå†…å­˜ï¼ˆML-20Mçš„ç”¨æˆ·IDåœ¨int32èŒƒå›´å†…ï¼‰
                item_col: "int32",  # ä½¿ç”¨int32èŠ‚çœå†…å­˜
            },
            engine="c",
            low_memory=False,
            na_filter=False,
            chunksize=chunk_size,
        )
        
        # æ·»åŠ è¿›åº¦æ˜¾ç¤º
        if show_progress and tqdm is not None:
            chunk_iter = tqdm(chunk_iter, desc=f"è¯»å– {p.name}", unit="å—", leave=False)
        
        try:
            for chunk in chunk_iter:
                chunks.append(chunk)
            # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„concatæ–¹å¼
            if len(chunks) == 1:
                df = chunks[0]
            else:
                df = pd.concat(chunks, ignore_index=True, copy=False)
        except Exception:
            # å¦‚æœåˆ†å—è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†è¯»å–
            if show_progress:
                print(f"è­¦å‘Š: æ–‡ä»¶ {p.name} åˆ†å—è¯»å–å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†è¯»å–")
            df = pd.read_csv(
                p,
                usecols=[user_col, item_col],
                dtype={
                    user_col: "int32",
                    item_col: "int32",
                },
                engine="c",
                low_memory=False,
                na_filter=False,
            )
    else:
        try:
            # å°è¯•ä½¿ç”¨pyarrowï¼ˆæ›´å¿«ï¼‰
            if HAS_PYARROW:
                try:
                    table = pacsv.read_csv(
                        p,
                        convert_options=pacsv.ConvertOptions(
                            include_columns=[user_col, item_col],
                            column_types={user_col: "int32", item_col: "int32"}
                        )
                    )
                    df = table.to_pandas()
                except Exception:
                    df = pd.read_csv(
                        p,
                        usecols=[user_col, item_col],
                        dtype={
                            user_col: "int32",
                            item_col: "int32",
                        },
                        engine="c",
                        low_memory=False,
                        na_filter=False,
                    )
            else:
                df = pd.read_csv(
                    p,
                    usecols=[user_col, item_col],
                    dtype={
                        user_col: "int32",
                        item_col: "int32",
                    },
                    engine="c",
                    low_memory=False,
                    na_filter=False,
                )
        except KeyError:
            # å¦‚æœåˆ—åä¸åŒ¹é…ï¼Œå°è¯•è¯»å–æ‰€æœ‰åˆ—ç„¶åç­›é€‰
            df = pd.read_csv(p, engine="c", low_memory=False)
            if user_col not in df.columns or item_col not in df.columns:
                raise ValueError(f"æ–‡ä»¶ {p} ç¼ºå°‘å¿…éœ€çš„åˆ—: {user_col}, {item_col}")
            df = df[[user_col, item_col]]
            df[user_col] = df[user_col].astype("int32")
            df[item_col] = df[item_col].astype("int32")
        except Exception as e:
            # å¦‚æœä¼˜åŒ–è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†è¯»å–
            if show_progress:
                print(f"è­¦å‘Š: æ–‡ä»¶ {p.name} ä½¿ç”¨æ ‡å‡†è¯»å–æ–¹å¼: {e}")
            df = pd.read_csv(p)
            if user_col not in df.columns or item_col not in df.columns:
                raise ValueError(f"æ–‡ä»¶ {p} ç¼ºå°‘å¿…éœ€çš„åˆ—: {user_col}, {item_col}")
            df = df[[user_col, item_col]]
            try:
                df[user_col] = df[user_col].astype("int32")
                df[item_col] = df[item_col].astype("int32")
            except Exception:
                pass
    
    df = df.rename(columns={user_col: "user_id", item_col: "item_id"})
    df["__source_file"] = p.name
    return df[["user_id", "item_id", "__source_file"]]


def _load_frames(paths: List[Path], show_progress: bool = False) -> pd.DataFrame:
    """åŠ è½½MovieLensæ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒå¹¶è¡Œè¯»å–ï¼‰ã€‚"""
    import multiprocessing as mp
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡ä»¶æˆ–æ–‡ä»¶æ•°é‡å°‘ï¼Œç›´æ¥ä¸²è¡Œè¯»å–ï¼ˆé¿å…è¿›ç¨‹å¼€é”€ï¼‰
    if len(paths) == 1:
        if show_progress and tqdm is not None:
            iterable = tqdm(paths, desc="è¯»å–CSVæ–‡ä»¶", unit="æ–‡ä»¶")
        else:
            iterable = paths
        frames = [_load_single_file(str(p), show_progress=show_progress) for p in iterable]
    elif len(paths) <= 3:
        # å°‘é‡æ–‡ä»¶ï¼šä¸²è¡Œè¯»å–ä½†æ˜¾ç¤ºè¿›åº¦
        frames = []
        iterable = paths
        if show_progress and tqdm is not None:
            iterable = tqdm(paths, desc="è¯»å–CSVæ–‡ä»¶", unit="æ–‡ä»¶")
        for p in iterable:
            frames.append(_load_single_file(str(p), show_progress=show_progress))
    else:
        # å¤šä¸ªæ–‡ä»¶ï¼šä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œè¯»å–
        num_workers = min(len(paths), mp.cpu_count())
        if show_progress:
            print(f"ä½¿ç”¨ {num_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œè¯»å– {len(paths)} ä¸ªæ–‡ä»¶...")
        
        # å°†Pathå¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿pickleåºåˆ—åŒ–
        path_strings = [str(p) for p in paths]
        with mp.Pool(processes=num_workers) as pool:
            frames = pool.map(_load_single_file, path_strings)
    
    if not frames:
        raise FileNotFoundError("åœ¨æ•°æ®ç›®å½•ä¸­æœªæ‰¾åˆ°å¯ç”¨çš„CSVæ–‡ä»¶ã€‚")
    
    if show_progress:
        print(f"åˆå¹¶ {len(frames)} ä¸ªæ•°æ®æ¡†...")
    # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„concatæ–¹å¼
    concatenated = pd.concat(frames, ignore_index=True, copy=False)
    return concatenated


def compute_user_item_matrix(
    df: pd.DataFrame,
    sample_users: Optional[int] = None,
    min_interactions: int = 5,
    show_progress: bool = False,
) -> Tuple[csr_matrix, np.ndarray, np.ndarray]:
    """
    æ„å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µï¼ˆç¨€ç–çŸ©é˜µï¼‰ã€‚
    
    Returns:
        user_item_matrix: ç¨€ç–çŸ©é˜µï¼Œshape=(n_users, n_items)
        user_ids: ç”¨æˆ·IDæ•°ç»„
        item_ids: ç‰©å“IDæ•°ç»„
    """
    required_cols = {"user_id", "item_id"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {sorted(missing)}")

    # è¿‡æ»¤äº¤äº’æ¬¡æ•°å¤ªå°‘çš„ç”¨æˆ·
    user_counts = df.groupby("user_id").size()
    valid_users = user_counts[user_counts >= min_interactions].index
    df_filtered = df[df["user_id"].isin(valid_users)].copy()

    if show_progress:
        print(f"è¿‡æ»¤åç”¨æˆ·æ•°: {len(valid_users):,} (åŸå§‹: {df['user_id'].nunique():,})")

    # é‡‡æ ·ç”¨æˆ·ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if sample_users is not None and sample_users < len(valid_users):
        sampled_user_ids = np.random.choice(
            valid_users, size=sample_users, replace=False
        )
        df_filtered = df_filtered[df_filtered["user_id"].isin(sampled_user_ids)]
        valid_users = sampled_user_ids
        if show_progress:
            print(f"é‡‡æ ·ç”¨æˆ·æ•°: {len(valid_users):,}")

    # è·å–æ¯ä¸ªç”¨æˆ·è®¿é—®çš„å”¯ä¸€ç‰©å“é›†åˆ
    user_items = df_filtered.groupby("user_id")["item_id"].apply(set).to_dict()

    # æ„å»ºç”¨æˆ·å’Œç‰©å“çš„æ˜ å°„
    unique_items = set()
    for items in user_items.values():
        unique_items.update(items)
    unique_items = sorted(unique_items)
    item_to_idx = {item: idx for idx, item in enumerate(unique_items)}
    user_to_idx = {user: idx for idx, user in enumerate(valid_users)}

    # æ„å»ºç¨€ç–çŸ©é˜µ
    rows = []
    cols = []
    data = []

    if show_progress:
        print("æ„å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µ...")
        iter_users = tqdm(user_items.items()) if tqdm else user_items.items()
    else:
        iter_users = user_items.items()

    for user_id, items in iter_users:
        user_idx = user_to_idx[user_id]
        for item_id in items:
            item_idx = item_to_idx[item_id]
            rows.append(user_idx)
            cols.append(item_idx)
            data.append(1.0)

    user_item_matrix = csr_matrix(
        (data, (rows, cols)), shape=(len(valid_users), len(unique_items))
    )

    user_ids = np.array(valid_users)
    item_ids = np.array(unique_items)

    return user_item_matrix, user_ids, item_ids


def compute_jaccard_similarity(
    user_item_matrix: csr_matrix, show_progress: bool = False
) -> np.ndarray:
    """
    è®¡ç®—ç”¨æˆ·ä¹‹é—´çš„Jaccardç›¸ä¼¼åº¦çŸ©é˜µã€‚
    
    Jaccardç›¸ä¼¼åº¦ = |A âˆ© B| / |A âˆª B|
    """
    n_users = user_item_matrix.shape[0]

    if show_progress:
        print(f"è®¡ç®— {n_users} ä¸ªç”¨æˆ·ä¹‹é—´çš„Jaccardç›¸ä¼¼åº¦...")

    # è®¡ç®—äº¤é›†ï¼šçŸ©é˜µä¹˜æ³•å¾—åˆ°äº¤é›†å¤§å°
    intersection = user_item_matrix.dot(user_item_matrix.T).toarray()

    # è®¡ç®—å¹¶é›†ï¼š|A âˆª B| = |A| + |B| - |A âˆ© B|
    user_sizes = np.array(user_item_matrix.sum(axis=1)).flatten()
    union = user_sizes[:, None] + user_sizes[None, :] - intersection

    # é¿å…é™¤ä»¥é›¶
    union = np.maximum(union, 1e-10)
    jaccard = intersection / union

    # å°†å¯¹è§’çº¿è®¾ä¸º1ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„ç›¸ä¼¼åº¦ï¼‰
    np.fill_diagonal(jaccard, 1.0)

    return jaccard


def compute_cosine_similarity(
    user_item_matrix: csr_matrix, show_progress: bool = False
) -> np.ndarray:
    """è®¡ç®—ç”¨æˆ·ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µã€‚"""
    n_users = user_item_matrix.shape[0]

    if show_progress:
        print(f"è®¡ç®— {n_users} ä¸ªç”¨æˆ·ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦...")

    # L2å½’ä¸€åŒ–
    norms = np.sqrt(np.array(user_item_matrix.power(2).sum(axis=1))).flatten()
    norms = np.maximum(norms, 1e-10)
    normalized_matrix = user_item_matrix.multiply(1.0 / norms[:, None])

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    cosine = normalized_matrix.dot(normalized_matrix.T).toarray()

    return cosine


def perform_kmeans_clustering(
    user_item_matrix: csr_matrix,
    similarity_matrix: np.ndarray,
    n_clusters: int,
    random_state: int = 42,
    compute_silhouette: bool = True,
    silhouette_sample_size: Optional[int] = None,
) -> Tuple[np.ndarray, Optional[float]]:
    """
    ä½¿ç”¨K-meanså¯¹ç”¨æˆ·è¿›è¡Œèšç±»ã€‚
    
    Args:
        compute_silhouette: æ˜¯å¦è®¡ç®—è½®å»“ç³»æ•°ï¼ˆå¤§è§„æ¨¡æ•°æ®å¯èƒ½å¾ˆæ…¢ï¼‰
        silhouette_sample_size: è®¡ç®—è½®å»“ç³»æ•°æ—¶çš„é‡‡æ ·å¤§å°ï¼ˆNoneè¡¨ç¤ºä¸é‡‡æ ·ï¼‰
    
    Returns:
        labels: èšç±»æ ‡ç­¾
        silhouette: è½®å»“ç³»æ•°ï¼ˆå¦‚æœcompute_silhouette=Falseåˆ™ä¸ºNoneï¼‰
    """
    n_users = user_item_matrix.shape[0]
    
    # ä½¿ç”¨PCAé™ç»´ä»¥æé«˜æ•ˆç‡ï¼ˆå¯¹äºå¤§é‡ç”¨æˆ·ï¼‰
    if n_users > 1000:
        # å¯¹äºå¤§è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨PCAé™ç»´
        n_components = min(50, n_users - 1)
        if n_users > 10000:
            # å¯¹äºè¶…å¤§è§„æ¨¡æ•°æ®ï¼Œè¿›ä¸€æ­¥å‡å°‘ç»„ä»¶æ•°
            n_components = min(30, n_users - 1)
        
        # ä½¿ç”¨ç¨€ç–çŸ©é˜µçš„SVDè¿›è¡ŒPCAï¼ˆæ›´é«˜æ•ˆï¼‰
        svd = TruncatedSVD(n_components=n_components, random_state=random_state)
        features = svd.fit_transform(user_item_matrix)
    else:
        features = similarity_matrix

    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)
    labels = kmeans.fit_predict(features)

    # è®¡ç®—è½®å»“ç³»æ•°ï¼ˆå¯é€‰ï¼Œå¤§è§„æ¨¡æ•°æ®å¯èƒ½å¾ˆæ…¢ï¼‰
    silhouette = None
    if compute_silhouette:
        # å¯¹äºå¤§è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨é‡‡æ ·æ–¹æ³•è®¡ç®—è½®å»“ç³»æ•°
        if silhouette_sample_size is not None and silhouette_sample_size < n_users:
            # é‡‡æ ·ç”¨æˆ·è®¡ç®—è½®å»“ç³»æ•°
            sample_indices = np.random.choice(
                n_users, size=silhouette_sample_size, replace=False
            )
            sample_labels = labels[sample_indices]
            sample_distance = 1 - similarity_matrix[np.ix_(sample_indices, sample_indices)]
            np.fill_diagonal(sample_distance, 0)
            silhouette = silhouette_score(
                sample_distance, sample_labels, metric="precomputed"
            )
        else:
            # è®¡ç®—å®Œæ•´è½®å»“ç³»æ•°
            distance_matrix = 1 - similarity_matrix
            np.fill_diagonal(distance_matrix, 0)
            silhouette = silhouette_score(distance_matrix, labels, metric="precomputed")

    return labels, silhouette


def perform_hierarchical_clustering(
    similarity_matrix: np.ndarray,
    n_clusters: int,
    method: str = "complete",
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ä½¿ç”¨å±‚æ¬¡èšç±»å¯¹ç”¨æˆ·è¿›è¡Œèšç±»ã€‚
    
    Args:
        similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ
        n_clusters: èšç±»æ•°é‡
        method: é“¾æ¥æ–¹æ³• ('complete', 'average', 'ward')
    """
    # å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºè·ç¦»
    distance_matrix = 1 - similarity_matrix
    np.fill_diagonal(distance_matrix, 0)

    # ä½¿ç”¨condensed distance matrix
    from scipy.spatial.distance import squareform
    condensed_dist = squareform(distance_matrix, checks=False)
    
    if method == "ward":
        linkage_matrix = linkage(condensed_dist, method=method)
    else:
        linkage_matrix = linkage(condensed_dist, method=method)

    # è·å–èšç±»æ ‡ç­¾
    labels = fcluster(linkage_matrix, n_clusters, criterion="maxclust") - 1

    return labels, linkage_matrix


def analyze_cluster_statistics(
    user_ids: np.ndarray,
    user_item_matrix: csr_matrix,
    labels: np.ndarray,
    similarity_matrix: np.ndarray,
) -> Dict:
    """åˆ†æèšç±»ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    n_clusters = len(np.unique(labels))
    cluster_stats = []

    for cluster_id in range(n_clusters):
        mask = labels == cluster_id
        cluster_users = user_ids[mask]
        cluster_size = len(cluster_users)

        # è®¡ç®—ç°‡å†…å¹³å‡ç›¸ä¼¼åº¦
        cluster_sim = similarity_matrix[mask][:, mask]
        intra_cluster_sim = cluster_sim[np.triu_indices(cluster_size, k=1)].mean()

        # è®¡ç®—ç°‡é—´å¹³å‡ç›¸ä¼¼åº¦
        inter_cluster_sims = []
        for other_cluster_id in range(n_clusters):
            if other_cluster_id != cluster_id:
                other_mask = labels == other_cluster_id
                inter_sim = similarity_matrix[mask][:, other_mask].mean()
                inter_cluster_sims.append(inter_sim)
        inter_cluster_sim = np.mean(inter_cluster_sims) if inter_cluster_sims else 0.0

        # è®¡ç®—ç°‡å†…ç”¨æˆ·è®¿é—®çš„ç‰©å“é›†åˆ
        cluster_items = user_item_matrix[mask].sum(axis=0).A1 > 0
        cluster_unique_items = cluster_items.sum()

        cluster_stats.append({
            "cluster_id": int(cluster_id),
            "size": int(cluster_size),
            "intra_cluster_similarity": float(intra_cluster_sim),
            "inter_cluster_similarity": float(inter_cluster_sim),
            "unique_items": int(cluster_unique_items),
            "user_ids": cluster_users.tolist(),
        })

    # è®¡ç®—æ•´ä½“ç»Ÿè®¡
    overall_intra_sim = np.mean([
        stat["intra_cluster_similarity"] for stat in cluster_stats
    ])
    overall_inter_sim = np.mean([
        stat["inter_cluster_similarity"] for stat in cluster_stats
    ])

    return {
        "n_clusters": n_clusters,
        "total_users": len(user_ids),
        "overall_intra_cluster_similarity": float(overall_intra_sim),
        "overall_inter_cluster_similarity": float(overall_inter_sim),
        "clustering_quality": float(overall_intra_sim - overall_inter_sim),
        "cluster_statistics": cluster_stats,
    }


def plot_similarity_heatmap(
    similarity_matrix: np.ndarray,
    output_path: Path,
    labels: Optional[np.ndarray] = None,
    max_users: int = 500,
) -> None:
    """ç»˜åˆ¶ç›¸ä¼¼åº¦çŸ©é˜µçƒ­åŠ›å›¾ã€‚"""
    if plt is None or sns is None:
        print("matplotlib/seabornæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    n_users = similarity_matrix.shape[0]

    # å¦‚æœç”¨æˆ·å¤ªå¤šï¼Œé‡‡æ ·æ˜¾ç¤º
    if n_users > max_users:
        indices = np.random.choice(n_users, size=max_users, replace=False)
        sim_subset = similarity_matrix[np.ix_(indices, indices)]
        labels_subset = labels[indices] if labels is not None else None
    else:
        sim_subset = similarity_matrix
        labels_subset = labels
        indices = np.arange(n_users)

    # å¦‚æœæœ‰æ ‡ç­¾ï¼ŒæŒ‰èšç±»æ’åº
    if labels_subset is not None:
        sort_order = np.argsort(labels_subset)
        sim_subset = sim_subset[np.ix_(sort_order, sort_order)]
        labels_subset = labels_subset[sort_order]

    fig, ax = plt.subplots(figsize=(12, 10))
    sns.heatmap(
        sim_subset,
        cmap="YlOrRd",
        square=True,
        cbar_kws={"label": "ç›¸ä¼¼åº¦"},
        ax=ax,
        vmin=0,
        vmax=1,
    )
    ax.set_title(f"ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µçƒ­åŠ›å›¾ (é‡‡æ · {len(sim_subset)} ä¸ªç”¨æˆ·)", fontsize=14)
    ax.set_xlabel("ç”¨æˆ·ç´¢å¼•")
    ax.set_ylabel("ç”¨æˆ·ç´¢å¼•")

    if labels_subset is not None:
        # æ·»åŠ èšç±»è¾¹ç•Œ
        unique_labels = np.unique(labels_subset)
        for label in unique_labels:
            mask = labels_subset == label
            boundary = np.where(mask)[0]
            if len(boundary) > 0:
                start = boundary[0]
                end = boundary[-1] + 1
                ax.axhline(start, color="blue", linewidth=2, alpha=0.7)
                ax.axhline(end, color="blue", linewidth=2, alpha=0.7)
                ax.axvline(start, color="blue", linewidth=2, alpha=0.7)
                ax.axvline(end, color="blue", linewidth=2, alpha=0.7)

    fig.tight_layout()
    fig.savefig(output_path, dpi=200, bbox_inches="tight")
    plt.close(fig)


def plot_cluster_distribution(
    labels: np.ndarray,
    similarity_matrix: np.ndarray,
    output_path: Path,
) -> None:
    """ç»˜åˆ¶èšç±»åˆ†å¸ƒå’Œç›¸ä¼¼åº¦åˆ†å¸ƒã€‚"""
    if plt is None:
        print("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # èšç±»å¤§å°åˆ†å¸ƒ
    unique_labels, counts = np.unique(labels, return_counts=True)
    axes[0].bar(unique_labels, counts, alpha=0.7, color="steelblue")
    axes[0].set_xlabel("èšç±»ID")
    axes[0].set_ylabel("ç”¨æˆ·æ•°é‡")
    axes[0].set_title("å„èšç±»çš„ç”¨æˆ·æ•°é‡åˆ†å¸ƒ")
    axes[0].grid(True, alpha=0.3)

    # ç°‡å†…å’Œç°‡é—´ç›¸ä¼¼åº¦åˆ†å¸ƒ
    n_clusters = len(unique_labels)
    intra_sims = []
    inter_sims = []

    for cluster_id in unique_labels:
        mask = labels == cluster_id
        cluster_sim = similarity_matrix[mask][:, mask]
        intra_sims.extend(
            cluster_sim[np.triu_indices(np.sum(mask), k=1)].tolist()
        )

        for other_cluster_id in unique_labels:
            if other_cluster_id != cluster_id:
                other_mask = labels == other_cluster_id
                inter_sim = similarity_matrix[mask][:, other_mask]
                inter_sims.extend(inter_sim.flatten().tolist())

    axes[1].hist(
        intra_sims,
        bins=50,
        alpha=0.6,
        label=f"ç°‡å†…ç›¸ä¼¼åº¦ (å‡å€¼={np.mean(intra_sims):.3f})",
        color="green",
    )
    axes[1].hist(
        inter_sims,
        bins=50,
        alpha=0.6,
        label=f"ç°‡é—´ç›¸ä¼¼åº¦ (å‡å€¼={np.mean(inter_sims):.3f})",
        color="red",
    )
    axes[1].set_xlabel("ç›¸ä¼¼åº¦")
    axes[1].set_ylabel("é¢‘æ•°")
    axes[1].set_title("ç°‡å†… vs ç°‡é—´ç›¸ä¼¼åº¦åˆ†å¸ƒ")
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    fig.tight_layout()
    fig.savefig(output_path, dpi=200, bbox_inches="tight")
    plt.close(fig)


def main() -> None:
    parser = argparse.ArgumentParser(description="åˆ†æMovieLensç”¨æˆ·èšç±»æ•ˆåº”")
    parser.add_argument(
        "--data-dir",
        type=Path,
        required=True,
        help="åŒ…å«MovieLensæ•°æ®CSVæ–‡ä»¶çš„ç›®å½•ã€‚",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("reports"),
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: reportsï¼‰ã€‚",
    )
    parser.add_argument(
        "--sample-users",
        type=int,
        default=None,
        help="é‡‡æ ·ç”¨æˆ·æ•°é‡ï¼ˆç”¨äºåŠ é€Ÿè®¡ç®—ï¼Œé»˜è®¤ä¸é‡‡æ ·ï¼‰ã€‚",
    )
    parser.add_argument(
        "--min-interactions",
        type=int,
        default=5,
        help="ç”¨æˆ·æœ€å°‘äº¤äº’æ¬¡æ•°ï¼ˆé»˜è®¤: 5ï¼‰ã€‚",
    )
    parser.add_argument(
        "--n-clusters",
        type=int,
        default=5,
        help="èšç±»æ•°é‡ï¼ˆé»˜è®¤: 5ï¼‰ã€‚",
    )
    parser.add_argument(
        "--clustering-method",
        type=str,
        choices=["kmeans", "hierarchical"],
        default="kmeans",
        help="èšç±»æ–¹æ³•ï¼ˆé»˜è®¤: kmeansï¼‰ã€‚",
    )
    parser.add_argument(
        "--similarity-metric",
        type=str,
        choices=["jaccard", "cosine"],
        default="jaccard",
        help="ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ï¼ˆé»˜è®¤: jaccardï¼‰ã€‚",
    )
    parser.add_argument(
        "--show-progress",
        action="store_true",
        help="æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯ã€‚",
    )
    parser.add_argument(
        "--max-heatmap-users",
        type=int,
        default=500,
        help="çƒ­åŠ›å›¾æœ€å¤§æ˜¾ç¤ºç”¨æˆ·æ•°ï¼ˆé»˜è®¤: 500ï¼‰ã€‚",
    )
    parser.add_argument(
        "--skip-silhouette",
        action="store_true",
        help="è·³è¿‡è½®å»“ç³»æ•°è®¡ç®—ï¼ˆå¤§è§„æ¨¡æ•°æ®æ—¶æ¨èä½¿ç”¨ï¼Œå¯æ˜¾è‘—åŠ é€Ÿï¼‰ã€‚",
    )
    parser.add_argument(
        "--silhouette-sample-size",
        type=int,
        default=None,
        help="è®¡ç®—è½®å»“ç³»æ•°æ—¶çš„é‡‡æ ·å¤§å°ï¼ˆé»˜è®¤ä¸é‡‡æ ·ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰ã€‚",
    )

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    args.output_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½æ•°æ®
    if args.show_progress:
        print("åŠ è½½æ•°æ®...")
    csvs = _find_csvs(args.data_dir)
    if not csvs:
        raise FileNotFoundError("æœªæ‰¾åˆ°CSVæ–‡ä»¶ã€‚æœŸæœ›SASRecæ ¼å¼çš„æ–‡ä»¶ã€‚")
    
    df = _load_frames(csvs, show_progress=args.show_progress)
    if args.show_progress:
        print(f"åŠ è½½äº† {len(df):,} æ¡äº¤äº’è®°å½•ï¼Œæ¥è‡ª {len(csvs)} ä¸ªæ–‡ä»¶ã€‚")

    # æ„å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µ
    user_item_matrix, user_ids, item_ids = compute_user_item_matrix(
        df,
        sample_users=args.sample_users,
        min_interactions=args.min_interactions,
        show_progress=args.show_progress,
    )

    n_users = user_item_matrix.shape[0]
    n_items = user_item_matrix.shape[1]
    
    if args.show_progress:
        print(
            f"ç”¨æˆ·-ç‰©å“çŸ©é˜µ: {n_users:,} ç”¨æˆ· Ã— "
            f"{n_items:,} ç‰©å“"
        )
        
        # å†…å­˜ä½¿ç”¨ä¼°ç®—å’Œè­¦å‘Š
        estimated_memory_gb = (n_users * n_users * 8) / (1024**3)  # float64
        print(
            f"é¢„è®¡ç›¸ä¼¼åº¦çŸ©é˜µå†…å­˜ä½¿ç”¨: ~{estimated_memory_gb:.2f} GB"
        )
        if estimated_memory_gb > 5:
            print(
                f"âš ï¸  è­¦å‘Š: ç›¸ä¼¼åº¦çŸ©é˜µè¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦è¾ƒå¤šå†…å­˜å’Œæ—¶é—´ã€‚"
            )
            if not args.skip_silhouette:
                print(
                    f"   å»ºè®®ä½¿ç”¨ --skip-silhouette è·³è¿‡è½®å»“ç³»æ•°è®¡ç®—ä»¥åŠ é€Ÿã€‚"
                )

    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    if args.similarity_metric == "jaccard":
        similarity_matrix = compute_jaccard_similarity(
            user_item_matrix, show_progress=args.show_progress
        )
    else:
        similarity_matrix = compute_cosine_similarity(
            user_item_matrix, show_progress=args.show_progress
        )

    if args.show_progress:
        # è®¡ç®—ç”¨æˆ·å¹³å‡è®¿é—®çš„ç‰©å“æ•°é‡
        user_item_counts = np.array(user_item_matrix.sum(axis=1)).flatten()
        avg_items_per_user = user_item_counts.mean()
        
        # æ’é™¤å¯¹è§’çº¿åçš„ç›¸ä¼¼åº¦ç»Ÿè®¡ï¼ˆå¯¹è§’çº¿éƒ½æ˜¯1ï¼Œä¼šå½±å“ç»Ÿè®¡ï¼‰
        mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
        off_diagonal_sim = similarity_matrix[mask]
        
        # è®¡ç®—åˆ†ä½æ•°
        percentiles = [25, 50, 75, 90, 95, 99]
        percentile_values = np.percentile(off_diagonal_sim, percentiles)
        
        # ç»Ÿè®¡é«˜ç›¸ä¼¼åº¦ç”¨æˆ·å¯¹çš„æ•°é‡
        high_sim_thresholds = [0.1, 0.2, 0.3, 0.5]
        high_sim_counts = {
            threshold: (off_diagonal_sim >= threshold).sum()
            for threshold in high_sim_thresholds
        }
        
        print(
            f"\nç›¸ä¼¼åº¦çŸ©é˜µç»Ÿè®¡:"
        )
        print(
            f"  ç”¨æˆ·å¹³å‡è®¿é—®ç‰©å“æ•°: {avg_items_per_user:.1f} "
            f"(èŒƒå›´: {user_item_counts.min():.0f} - {user_item_counts.max():.0f})"
        )
        print(
            f"  æ€»ç‰©å“æ•°: {n_items:,}"
        )
        print(
            f"  å®Œæ•´çŸ©é˜µç»Ÿè®¡ (åŒ…å«å¯¹è§’çº¿): "
            f"å‡å€¼={similarity_matrix.mean():.4f}, "
            f"ä¸­ä½æ•°={np.median(similarity_matrix):.4f}"
        )
        print(
            f"  éå¯¹è§’çº¿ç›¸ä¼¼åº¦ç»Ÿè®¡ (æ’é™¤è‡ªå·±): "
            f"å‡å€¼={off_diagonal_sim.mean():.4f}, "
            f"ä¸­ä½æ•°={np.median(off_diagonal_sim):.4f}, "
            f"æœ€å¤§å€¼={off_diagonal_sim.max():.4f}"
        )
        print(
            f"  ç›¸ä¼¼åº¦åˆ†ä½æ•°: "
            + ", ".join([
                f"P{p}={v:.4f}" 
                for p, v in zip(percentiles, percentile_values)
            ])
        )
        print(
            f"  é«˜ç›¸ä¼¼åº¦ç”¨æˆ·å¯¹æ•°é‡: "
            + ", ".join([
                f"â‰¥{t:.1f}: {cnt:,} ({cnt*100/len(off_diagonal_sim):.2f}%)"
                for t, cnt in high_sim_counts.items()
            ])
        )
        
        # åˆ¤æ–­æ˜¯å¦å­˜åœ¨èšç±»æ•ˆåº”
        if off_diagonal_sim.mean() < 0.01:
            print(
                f"\nâš ï¸  è­¦å‘Š: ç”¨æˆ·é—´å¹³å‡ç›¸ä¼¼åº¦å¾ˆä½ ({off_diagonal_sim.mean():.4f})ï¼Œ"
                f"å¯èƒ½å­˜åœ¨ä»¥ä¸‹æƒ…å†µï¼š"
            )
            print(
                f"  1. ç‰©å“ç©ºé—´å¾ˆå¤§ï¼Œç”¨æˆ·è®¿é—®çš„ç‰©å“é›†åˆé‡å å¾ˆå°"
            )
            print(
                f"  2. ç”¨æˆ·åå¥½å·®å¼‚å¾ˆå¤§ï¼Œä¸å­˜åœ¨æ˜æ˜¾çš„èšç±»æ•ˆåº”"
            )
            print(
                f"  3. å¯èƒ½éœ€è¦å¢åŠ é‡‡æ ·ç”¨æˆ·æ•°æˆ–è°ƒæ•´min-interactionså‚æ•°"
            )
        elif off_diagonal_sim.mean() > 0.1:
            print(
                f"\nâœ“ ç”¨æˆ·é—´å¹³å‡ç›¸ä¼¼åº¦è¾ƒé«˜ ({off_diagonal_sim.mean():.4f})ï¼Œ"
                f"å¯èƒ½å­˜åœ¨èšç±»æ•ˆåº”"
            )

    # éªŒè¯clusteræ•°é‡çš„åˆç†æ€§
    if args.n_clusters > n_users:
        raise ValueError(
            f"é”™è¯¯: clusteræ•°é‡ ({args.n_clusters}) ä¸èƒ½å¤§äºç”¨æˆ·æ•°é‡ ({n_users})"
        )
    if args.n_clusters > n_users / 2:
        print(
            f"\nâš ï¸  è­¦å‘Š: clusteræ•°é‡ ({args.n_clusters}) ç›¸å¯¹äºç”¨æˆ·æ•°é‡ ({n_users}) è¾ƒå¤šï¼Œ"
            f"å¹³å‡æ¯ä¸ªclusteråªæœ‰ {n_users/args.n_clusters:.1f} ä¸ªç”¨æˆ·ã€‚"
        )
        print(
            f"   è¿™å¯èƒ½å¯¼è‡´è¿‡åº¦åˆ†å‰²ï¼Œå»ºè®®clusteræ•°é‡ä¸è¶…è¿‡ç”¨æˆ·æ•°é‡çš„1/10 ({n_users//10})ã€‚"
        )
    elif args.n_clusters < n_users / 100:
        print(
            f"\nğŸ’¡ æç¤º: clusteræ•°é‡ ({args.n_clusters}) ç›¸å¯¹è¾ƒå°‘ï¼Œ"
            f"å¹³å‡æ¯ä¸ªclusteræœ‰ {n_users/args.n_clusters:.1f} ä¸ªç”¨æˆ·ã€‚"
        )
        print(
            f"   å¦‚æœç”¨æˆ·åå¥½å·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦æ›´å¤šclusteræ¥åŒºåˆ†ä¸åŒçš„ç”¨æˆ·ç¾¤ä½“ã€‚"
        )

    # æ‰§è¡Œèšç±»
    if args.clustering_method == "kmeans":
        labels, silhouette = perform_kmeans_clustering(
            user_item_matrix,
            similarity_matrix,
            args.n_clusters,
            compute_silhouette=not args.skip_silhouette,
            silhouette_sample_size=args.silhouette_sample_size,
        )
        linkage_matrix = None
        if args.show_progress:
            if silhouette is not None:
                print(f"K-meansèšç±»å®Œæˆï¼Œè½®å»“ç³»æ•°: {silhouette:.4f}")
            else:
                print(f"K-meansèšç±»å®Œæˆï¼ˆå·²è·³è¿‡è½®å»“ç³»æ•°è®¡ç®—ï¼‰")
    else:
        labels, linkage_matrix = perform_hierarchical_clustering(
            similarity_matrix,
            args.n_clusters,
        )
        # è®¡ç®—è½®å»“ç³»æ•°ï¼ˆå¯é€‰ï¼‰
        silhouette = None
        if not args.skip_silhouette:
            if args.silhouette_sample_size is not None and args.silhouette_sample_size < n_users:
                # é‡‡æ ·è®¡ç®—
                sample_indices = np.random.choice(
                    n_users, size=args.silhouette_sample_size, replace=False
                )
                sample_labels = labels[sample_indices]
                sample_distance = 1 - similarity_matrix[np.ix_(sample_indices, sample_indices)]
                np.fill_diagonal(sample_distance, 0)
                silhouette = silhouette_score(
                    sample_distance, sample_labels, metric="precomputed"
                )
            else:
                # å®Œæ•´è®¡ç®—
                distance_matrix = 1 - similarity_matrix
                np.fill_diagonal(distance_matrix, 0)
                silhouette = silhouette_score(
                    distance_matrix, labels, metric="precomputed"
                )
        if args.show_progress:
            if silhouette is not None:
                print(f"å±‚æ¬¡èšç±»å®Œæˆï¼Œè½®å»“ç³»æ•°: {silhouette:.4f}")
            else:
                print(f"å±‚æ¬¡èšç±»å®Œæˆï¼ˆå·²è·³è¿‡è½®å»“ç³»æ•°è®¡ç®—ï¼‰")

    # åˆ†æèšç±»ç»Ÿè®¡
    cluster_stats = analyze_cluster_statistics(
        user_ids, user_item_matrix, labels, similarity_matrix
    )

    if args.show_progress:
        # è®¡ç®—clusterå¤§å°ç»Ÿè®¡
        cluster_sizes = [stat["size"] for stat in cluster_stats["cluster_statistics"]]
        cluster_sizes_array = np.array(cluster_sizes)
        
        print(f"\nèšç±»åˆ†æç»“æœ:")
        print(f"  èšç±»æ•°é‡: {cluster_stats['n_clusters']}")
        print(f"  æ€»ç”¨æˆ·æ•°: {cluster_stats['total_users']}")
        print(f"  å¹³å‡æ¯ä¸ªclusterç”¨æˆ·æ•°: {np.mean(cluster_sizes_array):.1f}")
        print(f"  Clusterå¤§å°ç»Ÿè®¡: "
              f"æœ€å°={cluster_sizes_array.min()}, "
              f"æœ€å¤§={cluster_sizes_array.max()}, "
              f"ä¸­ä½æ•°={np.median(cluster_sizes_array):.1f}, "
              f"æ ‡å‡†å·®={cluster_sizes_array.std():.1f}")
        
        # ç»Ÿè®¡å°clusterï¼ˆå°‘äº5ä¸ªç”¨æˆ·ï¼‰çš„æ•°é‡
        small_clusters = (cluster_sizes_array < 5).sum()
        if small_clusters > 0:
            print(f"  âš ï¸  å°clusteræ•°é‡ï¼ˆ<5ç”¨æˆ·ï¼‰: {small_clusters} ({small_clusters*100/cluster_stats['n_clusters']:.1f}%)")
        
        print(f"  å¹³å‡ç°‡å†…ç›¸ä¼¼åº¦: {cluster_stats['overall_intra_cluster_similarity']:.4f}")
        print(f"  å¹³å‡ç°‡é—´ç›¸ä¼¼åº¦: {cluster_stats['overall_inter_cluster_similarity']:.4f}")
        print(f"  èšç±»è´¨é‡ (ç°‡å†…-ç°‡é—´): {cluster_stats['clustering_quality']:.4f}")
        if silhouette is not None:
            print(f"  è½®å»“ç³»æ•°: {silhouette:.4f}")
        else:
            print(f"  è½®å»“ç³»æ•°: æœªè®¡ç®—ï¼ˆä½¿ç”¨ --skip-silhouette è·³è¿‡ï¼‰")

        # åªæ˜¾ç¤ºå‰20ä¸ªclusterçš„è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœclusterå¤ªå¤šï¼‰
        max_display = 20
        if len(cluster_stats["cluster_statistics"]) > max_display:
            print(f"\nå„èšç±»ç»Ÿè®¡ï¼ˆæ˜¾ç¤ºå‰{max_display}ä¸ªï¼ŒæŒ‰ç”¨æˆ·æ•°æ’åºï¼‰:")
            sorted_stats = sorted(
                cluster_stats["cluster_statistics"],
                key=lambda x: x["size"],
                reverse=True
            )
            for stat in sorted_stats[:max_display]:
                print(
                    f"  èšç±» {stat['cluster_id']}: "
                    f"ç”¨æˆ·æ•°={stat['size']}, "
                    f"ç°‡å†…ç›¸ä¼¼åº¦={stat['intra_cluster_similarity']:.4f}, "
                    f"ç°‡é—´ç›¸ä¼¼åº¦={stat['inter_cluster_similarity']:.4f}, "
                    f"å”¯ä¸€ç‰©å“æ•°={stat['unique_items']}"
                )
            print(f"  ... (è¿˜æœ‰ {len(cluster_stats['cluster_statistics']) - max_display} ä¸ªclusteræœªæ˜¾ç¤º)")
        else:
            print(f"\nå„èšç±»ç»Ÿè®¡:")
            for stat in cluster_stats["cluster_statistics"]:
                print(
                    f"  èšç±» {stat['cluster_id']}: "
                    f"ç”¨æˆ·æ•°={stat['size']}, "
                    f"ç°‡å†…ç›¸ä¼¼åº¦={stat['intra_cluster_similarity']:.4f}, "
                    f"ç°‡é—´ç›¸ä¼¼åº¦={stat['inter_cluster_similarity']:.4f}, "
                    f"å”¯ä¸€ç‰©å“æ•°={stat['unique_items']}"
                )

    # ä¿å­˜ç»“æœ
    results = {
        "similarity_metric": args.similarity_metric,
        "clustering_method": args.clustering_method,
        "n_clusters": args.n_clusters,
        "silhouette_score": float(silhouette) if silhouette is not None else None,
        "cluster_statistics": cluster_stats,
        "user_cluster_mapping": {
            str(uid): int(label) for uid, label in zip(user_ids, labels)
        },
    }

    results_json_path = args.output_dir / "ml_user_clustering_results.json"
    results_json_path.write_text(
        json.dumps(results, indent=2, ensure_ascii=False)
    )
    if args.show_progress:
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_json_path}")

    # ç»˜åˆ¶å¯è§†åŒ–
    if plt is not None:
        # ç›¸ä¼¼åº¦çƒ­åŠ›å›¾
        heatmap_path = args.output_dir / "ml_user_similarity_heatmap.png"
        plot_similarity_heatmap(
            similarity_matrix,
            heatmap_path,
            labels=labels,
            max_users=args.max_heatmap_users,
        )
        if args.show_progress:
            print(f"ç›¸ä¼¼åº¦çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {heatmap_path}")

        # èšç±»åˆ†å¸ƒ
        cluster_dist_path = args.output_dir / "ml_cluster_distribution.png"
        plot_cluster_distribution(labels, similarity_matrix, cluster_dist_path)
        if args.show_progress:
            print(f"èšç±»åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {cluster_dist_path}")

    # ä¿å­˜ç”¨æˆ·èšç±»æ˜ å°„CSV
    user_cluster_df = pd.DataFrame({
        "user_id": user_ids,
        "cluster_id": labels,
    })
    user_cluster_csv_path = args.output_dir / "ml_user_cluster_mapping.csv"
    user_cluster_df.to_csv(user_cluster_csv_path, index=False)
    if args.show_progress:
        print(f"ç”¨æˆ·èšç±»æ˜ å°„å·²ä¿å­˜åˆ°: {user_cluster_csv_path}")


if __name__ == "__main__":
    main()

