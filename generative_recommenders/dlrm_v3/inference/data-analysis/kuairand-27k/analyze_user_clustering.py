#!/usr/bin/env python3
"""åˆ†æç”¨æˆ·ä¹‹é—´çš„itemé‡åˆåº¦å’Œèšç±»æ•ˆåº”ï¼Œç”¨äºGPUè°ƒåº¦ä¼˜åŒ–ã€‚"""
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    from matplotlib.ticker import FuncFormatter
except Exception:
    plt = None
    sns = None
    FuncFormatter = None

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None


def _load_log_frames(
    data_dir: Path, show_progress: bool
) -> Tuple[pd.DataFrame, List[Path]]:
    """åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰KuaiRandæ—¥å¿—CSVæ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰ã€‚"""
    log_paths = sorted(data_dir.glob("log_standard*.csv"))
    if not log_paths:
        msg = (
            "æœªæ‰¾åˆ°KuaiRandæ—¥å¿—æ–‡ä»¶ã€‚æœŸæœ›æ–‡ä»¶æ ¼å¼ï¼š"
            "'log_standard_4_08_to_4_21_1k.csv'ã€‚"
        )
        raise FileNotFoundError(msg)

    # åªè¯»å–éœ€è¦çš„åˆ—ï¼ˆåŠ é€Ÿè¯»å–ï¼‰
    usecols = ["user_id", "video_id"]  # åªè¯»å–èšç±»åˆ†æéœ€è¦çš„åˆ—
    
    iterable = log_paths
    if show_progress and tqdm is not None:
        iterable = tqdm(log_paths, desc="è¯»å–æ—¥å¿—", unit="æ–‡ä»¶")
    elif show_progress and tqdm is None:
        print("tqdmæœªå®‰è£…ï¼›ç»§ç»­æ‰§è¡Œä½†ä¸æ˜¾ç¤ºè¿›åº¦æ¡ã€‚")
        show_progress = False

    frames = []
    for csv_path in iterable:
        # ä¼˜åŒ–è¯»å–å‚æ•°
        try:
            # ä½¿ç”¨ä¼˜åŒ–çš„è¯»å–å‚æ•°
            frame = pd.read_csv(
                csv_path,
                usecols=usecols,
                dtype={
                    "user_id": "int64",
                    "video_id": "int64",
                },
                engine="c",  # Cå¼•æ“é€šå¸¸æœ€å¿«
                low_memory=False,  # é¿å…ç±»å‹æ¨æ–­çš„å¼€é”€
                na_filter=False,  # è·³è¿‡NAå€¼æ£€æŸ¥ï¼ˆå¦‚æœç¡®å®šæ²¡æœ‰NAï¼‰
            )
        except KeyError:
            # å¦‚æœæŸäº›æ–‡ä»¶æ²¡æœ‰è¿™äº›åˆ—ï¼Œå°è¯•è¯»å–æ‰€æœ‰åˆ—
            frame = pd.read_csv(
                csv_path,
                dtype={
                    "user_id": "int64",
                    "video_id": "int64",
                },
                engine="c",
                low_memory=False,
            )
            # åªä¿ç•™éœ€è¦çš„åˆ—
            if "user_id" in frame.columns and "video_id" in frame.columns:
                frame = frame[["user_id", "video_id"]]
            else:
                raise ValueError(f"æ–‡ä»¶ {csv_path} ç¼ºå°‘å¿…éœ€çš„åˆ—: user_id, video_id")
        except Exception as e:
            # å¦‚æœä¼˜åŒ–è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†è¯»å–
            if show_progress:
                print(f"è­¦å‘Š: æ–‡ä»¶ {csv_path.name} ä½¿ç”¨æ ‡å‡†è¯»å–æ–¹å¼: {e}")
            frame = pd.read_csv(csv_path)
            if "user_id" in frame.columns and "video_id" in frame.columns:
                frame = frame[["user_id", "video_id"]]
            else:
                raise ValueError(f"æ–‡ä»¶ {csv_path} ç¼ºå°‘å¿…éœ€çš„åˆ—: user_id, video_id")
        
        frame["__source_file"] = csv_path.name
        frames.append(frame)

    if show_progress:
        print(f"åˆå¹¶ {len(frames)} ä¸ªæ•°æ®æ¡†...")
    concatenated = pd.concat(frames, ignore_index=True)
    return concatenated, log_paths


def compute_user_item_matrix(
    df: pd.DataFrame,
    sample_users: Optional[int] = None,
    min_interactions: int = 5,
    show_progress: bool = False,
) -> Tuple[csr_matrix, np.ndarray, np.ndarray]:
    """
    æ„å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µï¼ˆç¨€ç–çŸ©é˜µï¼‰ã€‚
    
    Returns:
        user_item_matrix: ç¨€ç–çŸ©é˜µï¼Œshape=(n_users, n_items)
        user_ids: ç”¨æˆ·IDæ•°ç»„
        item_ids: ç‰©å“IDæ•°ç»„
    """
    required_cols = {"user_id", "video_id"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {sorted(missing)}")

    # è¿‡æ»¤äº¤äº’æ¬¡æ•°å¤ªå°‘çš„ç”¨æˆ·
    user_counts = df.groupby("user_id").size()
    valid_users = user_counts[user_counts >= min_interactions].index
    df_filtered = df[df["user_id"].isin(valid_users)].copy()

    if show_progress:
        print(f"è¿‡æ»¤åç”¨æˆ·æ•°: {len(valid_users):,} (åŸå§‹: {df['user_id'].nunique():,})")

    # é‡‡æ ·ç”¨æˆ·ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if sample_users is not None and sample_users < len(valid_users):
        sampled_user_ids = np.random.choice(
            valid_users, size=sample_users, replace=False
        )
        df_filtered = df_filtered[df_filtered["user_id"].isin(sampled_user_ids)]
        valid_users = sampled_user_ids
        if show_progress:
            print(f"é‡‡æ ·ç”¨æˆ·æ•°: {len(valid_users):,}")

    # è·å–æ¯ä¸ªç”¨æˆ·è®¿é—®çš„å”¯ä¸€ç‰©å“é›†åˆ
    user_items = df_filtered.groupby("user_id")["video_id"].apply(set).to_dict()

    # æ„å»ºç”¨æˆ·å’Œç‰©å“çš„æ˜ å°„
    unique_items = set()
    for items in user_items.values():
        unique_items.update(items)
    unique_items = sorted(unique_items)
    item_to_idx = {item: idx for idx, item in enumerate(unique_items)}
    user_to_idx = {user: idx for idx, user in enumerate(valid_users)}

    # æ„å»ºç¨€ç–çŸ©é˜µ
    rows = []
    cols = []
    data = []

    if show_progress:
        print("æ„å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µ...")
        iter_users = tqdm(user_items.items()) if tqdm else user_items.items()
    else:
        iter_users = user_items.items()

    for user_id, items in iter_users:
        user_idx = user_to_idx[user_id]
        for item_id in items:
            item_idx = item_to_idx[item_id]
            rows.append(user_idx)
            cols.append(item_idx)
            data.append(1.0)

    user_item_matrix = csr_matrix(
        (data, (rows, cols)), shape=(len(valid_users), len(unique_items))
    )

    user_ids = np.array(valid_users)
    item_ids = np.array(unique_items)

    return user_item_matrix, user_ids, item_ids


def compute_jaccard_similarity(
    user_item_matrix: csr_matrix, show_progress: bool = False
) -> np.ndarray:
    """
    è®¡ç®—ç”¨æˆ·ä¹‹é—´çš„Jaccardç›¸ä¼¼åº¦çŸ©é˜µã€‚
    
    Jaccardç›¸ä¼¼åº¦ = |A âˆ© B| / |A âˆª B|
    """
    n_users = user_item_matrix.shape[0]

    if show_progress:
        print(f"è®¡ç®— {n_users} ä¸ªç”¨æˆ·ä¹‹é—´çš„Jaccardç›¸ä¼¼åº¦...")

    # è®¡ç®—äº¤é›†ï¼šçŸ©é˜µä¹˜æ³•å¾—åˆ°äº¤é›†å¤§å°
    intersection = user_item_matrix.dot(user_item_matrix.T).toarray()

    # è®¡ç®—å¹¶é›†ï¼š|A âˆª B| = |A| + |B| - |A âˆ© B|
    user_sizes = np.array(user_item_matrix.sum(axis=1)).flatten()
    union = user_sizes[:, None] + user_sizes[None, :] - intersection

    # é¿å…é™¤ä»¥é›¶
    union = np.maximum(union, 1e-10)
    jaccard = intersection / union

    # å°†å¯¹è§’çº¿è®¾ä¸º1ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„ç›¸ä¼¼åº¦ï¼‰
    np.fill_diagonal(jaccard, 1.0)

    return jaccard


def compute_cosine_similarity(
    user_item_matrix: csr_matrix, show_progress: bool = False
) -> np.ndarray:
    """è®¡ç®—ç”¨æˆ·ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µã€‚"""
    n_users = user_item_matrix.shape[0]

    if show_progress:
        print(f"è®¡ç®— {n_users} ä¸ªç”¨æˆ·ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦...")

    # L2å½’ä¸€åŒ–
    norms = np.sqrt(np.array(user_item_matrix.power(2).sum(axis=1))).flatten()
    norms = np.maximum(norms, 1e-10)
    normalized_matrix = user_item_matrix.multiply(1.0 / norms[:, None])

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    cosine = normalized_matrix.dot(normalized_matrix.T).toarray()

    return cosine


def perform_kmeans_clustering(
    user_item_matrix: csr_matrix,
    similarity_matrix: np.ndarray,
    n_clusters: int,
    random_state: int = 42,
    compute_silhouette: bool = True,
    silhouette_sample_size: Optional[int] = None,
) -> Tuple[np.ndarray, Optional[float]]:
    """
    ä½¿ç”¨K-meanså¯¹ç”¨æˆ·è¿›è¡Œèšç±»ã€‚
    
    Args:
        compute_silhouette: æ˜¯å¦è®¡ç®—è½®å»“ç³»æ•°ï¼ˆå¤§è§„æ¨¡æ•°æ®å¯èƒ½å¾ˆæ…¢ï¼‰
        silhouette_sample_size: è®¡ç®—è½®å»“ç³»æ•°æ—¶çš„é‡‡æ ·å¤§å°ï¼ˆNoneè¡¨ç¤ºä¸é‡‡æ ·ï¼‰
    
    Returns:
        labels: èšç±»æ ‡ç­¾
        silhouette: è½®å»“ç³»æ•°ï¼ˆå¦‚æœcompute_silhouette=Falseåˆ™ä¸ºNoneï¼‰
    """
    n_users = user_item_matrix.shape[0]
    
    # ä½¿ç”¨PCAé™ç»´ä»¥æé«˜æ•ˆç‡ï¼ˆå¯¹äºå¤§é‡ç”¨æˆ·ï¼‰
    if n_users > 1000:
        # å¯¹äºå¤§è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨PCAé™ç»´
        n_components = min(50, n_users - 1)
        if n_users > 10000:
            # å¯¹äºè¶…å¤§è§„æ¨¡æ•°æ®ï¼Œè¿›ä¸€æ­¥å‡å°‘ç»„ä»¶æ•°
            n_components = min(30, n_users - 1)
        
        # ä½¿ç”¨ç¨€ç–çŸ©é˜µçš„SVDè¿›è¡ŒPCAï¼ˆæ›´é«˜æ•ˆï¼‰
        from sklearn.decomposition import TruncatedSVD
        svd = TruncatedSVD(n_components=n_components, random_state=random_state)
        features = svd.fit_transform(user_item_matrix)
    else:
        features = similarity_matrix

    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)
    labels = kmeans.fit_predict(features)

    # è®¡ç®—è½®å»“ç³»æ•°ï¼ˆå¯é€‰ï¼Œå¤§è§„æ¨¡æ•°æ®å¯èƒ½å¾ˆæ…¢ï¼‰
    silhouette = None
    if compute_silhouette:
        # å¯¹äºå¤§è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨é‡‡æ ·æ–¹æ³•è®¡ç®—è½®å»“ç³»æ•°
        if silhouette_sample_size is not None and silhouette_sample_size < n_users:
            # é‡‡æ ·ç”¨æˆ·è®¡ç®—è½®å»“ç³»æ•°
            sample_indices = np.random.choice(
                n_users, size=silhouette_sample_size, replace=False
            )
            sample_labels = labels[sample_indices]
            sample_distance = 1 - similarity_matrix[np.ix_(sample_indices, sample_indices)]
            np.fill_diagonal(sample_distance, 0)
            silhouette = silhouette_score(
                sample_distance, sample_labels, metric="precomputed"
            )
        else:
            # è®¡ç®—å®Œæ•´è½®å»“ç³»æ•°
            distance_matrix = 1 - similarity_matrix
            np.fill_diagonal(distance_matrix, 0)
            silhouette = silhouette_score(distance_matrix, labels, metric="precomputed")

    return labels, silhouette


def perform_hierarchical_clustering(
    similarity_matrix: np.ndarray,
    n_clusters: int,
    method: str = "complete",
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ä½¿ç”¨å±‚æ¬¡èšç±»å¯¹ç”¨æˆ·è¿›è¡Œèšç±»ã€‚
    
    Args:
        similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ
        n_clusters: èšç±»æ•°é‡
        method: é“¾æ¥æ–¹æ³• ('complete', 'average', 'ward')
        æ³¨æ„: 'ward' æ–¹æ³•éœ€è¦æ¬§æ°è·ç¦»ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨å¹³æ–¹æ¬§æ°è·ç¦»
    """
    # å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºè·ç¦»
    distance_matrix = 1 - similarity_matrix
    np.fill_diagonal(distance_matrix, 0)

    # wardæ–¹æ³•éœ€è¦æ¬§æ°è·ç¦»ï¼Œä½¿ç”¨å¹³æ–¹æ¬§æ°è·ç¦»
    if method == "ward":
        # å°†è·ç¦»çŸ©é˜µè½¬æ¢ä¸ºå¹³æ–¹æ¬§æ°è·ç¦»å½¢å¼
        # ä½¿ç”¨ condensed distance matrix
        from scipy.spatial.distance import squareform
        condensed_dist = squareform(distance_matrix, checks=False)
        linkage_matrix = linkage(condensed_dist, method=method)
    else:
        # å¯¹äºå…¶ä»–æ–¹æ³•ï¼Œä½¿ç”¨condensed distance matrix
        from scipy.spatial.distance import squareform
        condensed_dist = squareform(distance_matrix, checks=False)
        linkage_matrix = linkage(condensed_dist, method=method)

    # è·å–èšç±»æ ‡ç­¾
    labels = fcluster(linkage_matrix, n_clusters, criterion="maxclust") - 1

    return labels, linkage_matrix


def analyze_cluster_statistics(
    user_ids: np.ndarray,
    user_item_matrix: csr_matrix,
    labels: np.ndarray,
    similarity_matrix: np.ndarray,
) -> Dict:
    """åˆ†æèšç±»ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    n_clusters = len(np.unique(labels))
    cluster_stats = []

    for cluster_id in range(n_clusters):
        mask = labels == cluster_id
        cluster_users = user_ids[mask]
        cluster_size = len(cluster_users)

        # è®¡ç®—ç°‡å†…å¹³å‡ç›¸ä¼¼åº¦
        cluster_sim = similarity_matrix[mask][:, mask]
        intra_cluster_sim = cluster_sim[np.triu_indices(cluster_size, k=1)].mean()

        # è®¡ç®—ç°‡é—´å¹³å‡ç›¸ä¼¼åº¦
        inter_cluster_sims = []
        for other_cluster_id in range(n_clusters):
            if other_cluster_id != cluster_id:
                other_mask = labels == other_cluster_id
                inter_sim = similarity_matrix[mask][:, other_mask].mean()
                inter_cluster_sims.append(inter_sim)
        inter_cluster_sim = np.mean(inter_cluster_sims) if inter_cluster_sims else 0.0

        # è®¡ç®—ç°‡å†…ç”¨æˆ·è®¿é—®çš„ç‰©å“é›†åˆ
        cluster_items = user_item_matrix[mask].sum(axis=0).A1 > 0
        cluster_unique_items = cluster_items.sum()

        cluster_stats.append({
            "cluster_id": int(cluster_id),
            "size": int(cluster_size),
            "intra_cluster_similarity": float(intra_cluster_sim),
            "inter_cluster_similarity": float(inter_cluster_sim),
            "unique_items": int(cluster_unique_items),
            "user_ids": cluster_users.tolist(),
        })

    # è®¡ç®—æ•´ä½“ç»Ÿè®¡
    overall_intra_sim = np.mean([
        stat["intra_cluster_similarity"] for stat in cluster_stats
    ])
    overall_inter_sim = np.mean([
        stat["inter_cluster_similarity"] for stat in cluster_stats
    ])

    return {
        "n_clusters": n_clusters,
        "total_users": len(user_ids),
        "overall_intra_cluster_similarity": float(overall_intra_sim),
        "overall_inter_cluster_similarity": float(overall_inter_sim),
        "clustering_quality": float(overall_intra_sim - overall_inter_sim),
        "cluster_statistics": cluster_stats,
    }


def plot_similarity_heatmap(
    similarity_matrix: np.ndarray,
    output_path: Path,
    labels: Optional[np.ndarray] = None,
    max_users: int = 500,
) -> None:
    """ç»˜åˆ¶ç›¸ä¼¼åº¦çŸ©é˜µçƒ­åŠ›å›¾ã€‚"""
    if plt is None or sns is None:
        print("matplotlib/seabornæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    n_users = similarity_matrix.shape[0]

    # å¦‚æœç”¨æˆ·å¤ªå¤šï¼Œé‡‡æ ·æ˜¾ç¤º
    if n_users > max_users:
        indices = np.random.choice(n_users, size=max_users, replace=False)
        sim_subset = similarity_matrix[np.ix_(indices, indices)]
        labels_subset = labels[indices] if labels is not None else None
    else:
        sim_subset = similarity_matrix
        labels_subset = labels
        indices = np.arange(n_users)

    # å¦‚æœæœ‰æ ‡ç­¾ï¼ŒæŒ‰èšç±»æ’åº
    if labels_subset is not None:
        sort_order = np.argsort(labels_subset)
        sim_subset = sim_subset[np.ix_(sort_order, sort_order)]
        labels_subset = labels_subset[sort_order]

    fig, ax = plt.subplots(figsize=(12, 10))
    sns.heatmap(
        sim_subset,
        cmap="YlOrRd",
        square=True,
        cbar_kws={"label": "ç›¸ä¼¼åº¦"},
        ax=ax,
        vmin=0,
        vmax=1,
    )
    ax.set_title(f"ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µçƒ­åŠ›å›¾ (é‡‡æ · {len(sim_subset)} ä¸ªç”¨æˆ·)", fontsize=14)
    ax.set_xlabel("ç”¨æˆ·ç´¢å¼•")
    ax.set_ylabel("ç”¨æˆ·ç´¢å¼•")

    if labels_subset is not None:
        # æ·»åŠ èšç±»è¾¹ç•Œ
        unique_labels = np.unique(labels_subset)
        for label in unique_labels:
            mask = labels_subset == label
            boundary = np.where(mask)[0]
            if len(boundary) > 0:
                start = boundary[0]
                end = boundary[-1] + 1
                ax.axhline(start, color="blue", linewidth=2, alpha=0.7)
                ax.axhline(end, color="blue", linewidth=2, alpha=0.7)
                ax.axvline(start, color="blue", linewidth=2, alpha=0.7)
                ax.axvline(end, color="blue", linewidth=2, alpha=0.7)

    fig.tight_layout()
    fig.savefig(output_path, dpi=200, bbox_inches="tight")
    plt.close(fig)


def plot_cluster_distribution(
    labels: np.ndarray,
    similarity_matrix: np.ndarray,
    output_path: Path,
) -> None:
    """ç»˜åˆ¶èšç±»åˆ†å¸ƒå’Œç›¸ä¼¼åº¦åˆ†å¸ƒã€‚"""
    if plt is None:
        print("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # èšç±»å¤§å°åˆ†å¸ƒ
    unique_labels, counts = np.unique(labels, return_counts=True)
    axes[0].bar(unique_labels, counts, alpha=0.7, color="steelblue")
    axes[0].set_xlabel("èšç±»ID")
    axes[0].set_ylabel("ç”¨æˆ·æ•°é‡")
    axes[0].set_title("å„èšç±»çš„ç”¨æˆ·æ•°é‡åˆ†å¸ƒ")
    axes[0].grid(True, alpha=0.3)

    # ç°‡å†…å’Œç°‡é—´ç›¸ä¼¼åº¦åˆ†å¸ƒ
    n_clusters = len(unique_labels)
    intra_sims = []
    inter_sims = []

    for cluster_id in unique_labels:
        mask = labels == cluster_id
        cluster_sim = similarity_matrix[mask][:, mask]
        intra_sims.extend(
            cluster_sim[np.triu_indices(np.sum(mask), k=1)].tolist()
        )

        for other_cluster_id in unique_labels:
            if other_cluster_id != cluster_id:
                other_mask = labels == other_cluster_id
                inter_sim = similarity_matrix[mask][:, other_mask]
                inter_sims.extend(inter_sim.flatten().tolist())

    axes[1].hist(
        intra_sims,
        bins=50,
        alpha=0.6,
        label=f"ç°‡å†…ç›¸ä¼¼åº¦ (å‡å€¼={np.mean(intra_sims):.3f})",
        color="green",
    )
    axes[1].hist(
        inter_sims,
        bins=50,
        alpha=0.6,
        label=f"ç°‡é—´ç›¸ä¼¼åº¦ (å‡å€¼={np.mean(inter_sims):.3f})",
        color="red",
    )
    axes[1].set_xlabel("ç›¸ä¼¼åº¦")
    axes[1].set_ylabel("é¢‘æ•°")
    axes[1].set_title("ç°‡å†… vs ç°‡é—´ç›¸ä¼¼åº¦åˆ†å¸ƒ")
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    fig.tight_layout()
    fig.savefig(output_path, dpi=200, bbox_inches="tight")
    plt.close(fig)


def plot_dendrogram(
    linkage_matrix: np.ndarray,
    output_path: Path,
    max_display: int = 100,
) -> None:
    """ç»˜åˆ¶å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾ã€‚"""
    if plt is None:
        print("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    fig, ax = plt.subplots(figsize=(15, 8))
    dendrogram(
        linkage_matrix,
        truncate_mode="lastp",
        p=max_display,
        leaf_rotation=90,
        leaf_font_size=8,
        ax=ax,
    )
    ax.set_title("ç”¨æˆ·å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾", fontsize=14)
    ax.set_xlabel("ç”¨æˆ·ç´¢å¼•")
    ax.set_ylabel("è·ç¦»")
    fig.tight_layout()
    fig.savefig(output_path, dpi=200, bbox_inches="tight")
    plt.close(fig)


def main() -> None:
    parser = argparse.ArgumentParser(description="åˆ†æç”¨æˆ·èšç±»æ•ˆåº”")
    parser.add_argument(
        "--data-dir",
        type=Path,
        help="åŒ…å«KuaiRandæ—¥å¿—CSVæ–‡ä»¶çš„ç›®å½•ã€‚",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("reports"),
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: reportsï¼‰ã€‚",
    )
    parser.add_argument(
        "--sample-users",
        type=int,
        default=None,
        help="é‡‡æ ·ç”¨æˆ·æ•°é‡ï¼ˆç”¨äºåŠ é€Ÿè®¡ç®—ï¼Œé»˜è®¤ä¸é‡‡æ ·ï¼‰ã€‚",
    )
    parser.add_argument(
        "--min-interactions",
        type=int,
        default=5,
        help="ç”¨æˆ·æœ€å°‘äº¤äº’æ¬¡æ•°ï¼ˆé»˜è®¤: 5ï¼‰ã€‚",
    )
    parser.add_argument(
        "--n-clusters",
        type=int,
        default=5,
        help="èšç±»æ•°é‡ï¼ˆé»˜è®¤: 5ï¼‰ã€‚",
    )
    parser.add_argument(
        "--clustering-method",
        type=str,
        choices=["kmeans", "hierarchical"],
        default="kmeans",
        help="èšç±»æ–¹æ³•ï¼ˆé»˜è®¤: kmeansï¼‰ã€‚",
    )
    parser.add_argument(
        "--similarity-metric",
        type=str,
        choices=["jaccard", "cosine"],
        default="jaccard",
        help="ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ï¼ˆé»˜è®¤: jaccardï¼‰ã€‚",
    )
    parser.add_argument(
        "--show-progress",
        action="store_true",
        help="æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯ã€‚",
    )
    parser.add_argument(
        "--max-heatmap-users",
        type=int,
        default=500,
        help="çƒ­åŠ›å›¾æœ€å¤§æ˜¾ç¤ºç”¨æˆ·æ•°ï¼ˆé»˜è®¤: 500ï¼‰ã€‚",
    )
    parser.add_argument(
        "--skip-silhouette",
        action="store_true",
        help="è·³è¿‡è½®å»“ç³»æ•°è®¡ç®—ï¼ˆå¤§è§„æ¨¡æ•°æ®æ—¶æ¨èä½¿ç”¨ï¼Œå¯æ˜¾è‘—åŠ é€Ÿï¼‰ã€‚",
    )
    parser.add_argument(
        "--silhouette-sample-size",
        type=int,
        default=None,
        help="è®¡ç®—è½®å»“ç³»æ•°æ—¶çš„é‡‡æ ·å¤§å°ï¼ˆé»˜è®¤ä¸é‡‡æ ·ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰ã€‚",
    )

    args = parser.parse_args()

    # ç¡®å®šæ•°æ®ç›®å½•
    default_data_dir = (
        Path(__file__).resolve().parents[1] / "data" / "KuaiRand-1K" / "data"
    )
    data_dir = (args.data_dir or default_data_dir).expanduser()
    if not data_dir.exists():
        raise FileNotFoundError(
            f"æ•°æ®ç›®å½• '{data_dir}' ä¸å­˜åœ¨ã€‚"
            "è¯·ä½¿ç”¨ --data-dir æŒ‡å®šåŒ…å«KuaiRand CSVæ–‡ä»¶çš„ç›®å½•ã€‚"
        )

    # åˆ›å»ºè¾“å‡ºç›®å½•
    args.output_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½æ•°æ®
    if args.show_progress:
        print("åŠ è½½æ•°æ®...")
    df, log_paths = _load_log_frames(data_dir, args.show_progress)
    if args.show_progress:
        print(f"åŠ è½½äº† {len(df):,} æ¡äº¤äº’è®°å½•ï¼Œæ¥è‡ª {len(log_paths)} ä¸ªæ—¥å¿—æ–‡ä»¶ã€‚")

    # æ„å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µ
    user_item_matrix, user_ids, item_ids = compute_user_item_matrix(
        df,
        sample_users=args.sample_users,
        min_interactions=args.min_interactions,
        show_progress=args.show_progress,
    )

    n_users = user_item_matrix.shape[0]
    n_items = user_item_matrix.shape[1]
    
    if args.show_progress:
        print(
            f"ç”¨æˆ·-ç‰©å“çŸ©é˜µ: {n_users:,} ç”¨æˆ· Ã— "
            f"{n_items:,} ç‰©å“"
        )
        
        # å†…å­˜ä½¿ç”¨ä¼°ç®—å’Œè­¦å‘Š
        estimated_memory_gb = (n_users * n_users * 8) / (1024**3)  # float64
        print(
            f"é¢„è®¡ç›¸ä¼¼åº¦çŸ©é˜µå†…å­˜ä½¿ç”¨: ~{estimated_memory_gb:.2f} GB"
        )
        if estimated_memory_gb > 5:
            print(
                f"âš ï¸  è­¦å‘Š: ç›¸ä¼¼åº¦çŸ©é˜µè¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦è¾ƒå¤šå†…å­˜å’Œæ—¶é—´ã€‚"
            )
            if not args.skip_silhouette:
                print(
                    f"   å»ºè®®ä½¿ç”¨ --skip-silhouette è·³è¿‡è½®å»“ç³»æ•°è®¡ç®—ä»¥åŠ é€Ÿã€‚"
                )

    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    if args.similarity_metric == "jaccard":
        similarity_matrix = compute_jaccard_similarity(
            user_item_matrix, show_progress=args.show_progress
        )
    else:
        similarity_matrix = compute_cosine_similarity(
            user_item_matrix, show_progress=args.show_progress
        )

    if args.show_progress:
        # è®¡ç®—ç”¨æˆ·å¹³å‡è®¿é—®çš„ç‰©å“æ•°é‡
        user_item_counts = np.array(user_item_matrix.sum(axis=1)).flatten()
        avg_items_per_user = user_item_counts.mean()
        
        # æ’é™¤å¯¹è§’çº¿åçš„ç›¸ä¼¼åº¦ç»Ÿè®¡ï¼ˆå¯¹è§’çº¿éƒ½æ˜¯1ï¼Œä¼šå½±å“ç»Ÿè®¡ï¼‰
        mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
        off_diagonal_sim = similarity_matrix[mask]
        
        # è®¡ç®—åˆ†ä½æ•°
        percentiles = [25, 50, 75, 90, 95, 99]
        percentile_values = np.percentile(off_diagonal_sim, percentiles)
        
        # ç»Ÿè®¡é«˜ç›¸ä¼¼åº¦ç”¨æˆ·å¯¹çš„æ•°é‡
        high_sim_thresholds = [0.1, 0.2, 0.3, 0.5]
        high_sim_counts = {
            threshold: (off_diagonal_sim >= threshold).sum()
            for threshold in high_sim_thresholds
        }
        
        print(
            f"\nç›¸ä¼¼åº¦çŸ©é˜µç»Ÿè®¡:"
        )
        print(
            f"  ç”¨æˆ·å¹³å‡è®¿é—®ç‰©å“æ•°: {avg_items_per_user:.1f} "
            f"(èŒƒå›´: {user_item_counts.min():.0f} - {user_item_counts.max():.0f})"
        )
        print(
            f"  æ€»ç‰©å“æ•°: {user_item_matrix.shape[1]:,}"
        )
        print(
            f"  å®Œæ•´çŸ©é˜µç»Ÿè®¡ (åŒ…å«å¯¹è§’çº¿): "
            f"å‡å€¼={similarity_matrix.mean():.4f}, "
            f"ä¸­ä½æ•°={np.median(similarity_matrix):.4f}"
        )
        print(
            f"  éå¯¹è§’çº¿ç›¸ä¼¼åº¦ç»Ÿè®¡ (æ’é™¤è‡ªå·±): "
            f"å‡å€¼={off_diagonal_sim.mean():.4f}, "
            f"ä¸­ä½æ•°={np.median(off_diagonal_sim):.4f}, "
            f"æœ€å¤§å€¼={off_diagonal_sim.max():.4f}"
        )
        print(
            f"  ç›¸ä¼¼åº¦åˆ†ä½æ•°: "
            + ", ".join([
                f"P{p}={v:.4f}" 
                for p, v in zip(percentiles, percentile_values)
            ])
        )
        print(
            f"  é«˜ç›¸ä¼¼åº¦ç”¨æˆ·å¯¹æ•°é‡: "
            + ", ".join([
                f"â‰¥{t:.1f}: {cnt:,} ({cnt*100/len(off_diagonal_sim):.2f}%)"
                for t, cnt in high_sim_counts.items()
            ])
        )
        
        # åˆ¤æ–­æ˜¯å¦å­˜åœ¨èšç±»æ•ˆåº”
        if off_diagonal_sim.mean() < 0.01:
            print(
                f"\nâš ï¸  è­¦å‘Š: ç”¨æˆ·é—´å¹³å‡ç›¸ä¼¼åº¦å¾ˆä½ ({off_diagonal_sim.mean():.4f})ï¼Œ"
                f"å¯èƒ½å­˜åœ¨ä»¥ä¸‹æƒ…å†µï¼š"
            )
            print(
                f"  1. ç‰©å“ç©ºé—´å¾ˆå¤§ï¼Œç”¨æˆ·è®¿é—®çš„ç‰©å“é›†åˆé‡å å¾ˆå°"
            )
            print(
                f"  2. ç”¨æˆ·åå¥½å·®å¼‚å¾ˆå¤§ï¼Œä¸å­˜åœ¨æ˜æ˜¾çš„èšç±»æ•ˆåº”"
            )
            print(
                f"  3. å¯èƒ½éœ€è¦å¢åŠ é‡‡æ ·ç”¨æˆ·æ•°æˆ–è°ƒæ•´min-interactionså‚æ•°"
            )
        elif off_diagonal_sim.mean() > 0.1:
            print(
                f"\nâœ“ ç”¨æˆ·é—´å¹³å‡ç›¸ä¼¼åº¦è¾ƒé«˜ ({off_diagonal_sim.mean():.4f})ï¼Œ"
                f"å¯èƒ½å­˜åœ¨èšç±»æ•ˆåº”"
            )

    # éªŒè¯clusteræ•°é‡çš„åˆç†æ€§
    n_users = user_item_matrix.shape[0]
    if args.n_clusters > n_users:
        raise ValueError(
            f"é”™è¯¯: clusteræ•°é‡ ({args.n_clusters}) ä¸èƒ½å¤§äºç”¨æˆ·æ•°é‡ ({n_users})"
        )
    if args.n_clusters > n_users / 2:
        print(
            f"\nâš ï¸  è­¦å‘Š: clusteræ•°é‡ ({args.n_clusters}) ç›¸å¯¹äºç”¨æˆ·æ•°é‡ ({n_users}) è¾ƒå¤šï¼Œ"
            f"å¹³å‡æ¯ä¸ªclusteråªæœ‰ {n_users/args.n_clusters:.1f} ä¸ªç”¨æˆ·ã€‚"
        )
        print(
            f"   è¿™å¯èƒ½å¯¼è‡´è¿‡åº¦åˆ†å‰²ï¼Œå»ºè®®clusteræ•°é‡ä¸è¶…è¿‡ç”¨æˆ·æ•°é‡çš„1/10 ({n_users//10})ã€‚"
        )
    elif args.n_clusters < n_users / 100:
        print(
            f"\nğŸ’¡ æç¤º: clusteræ•°é‡ ({args.n_clusters}) ç›¸å¯¹è¾ƒå°‘ï¼Œ"
            f"å¹³å‡æ¯ä¸ªclusteræœ‰ {n_users/args.n_clusters:.1f} ä¸ªç”¨æˆ·ã€‚"
        )
        print(
            f"   å¦‚æœç”¨æˆ·åå¥½å·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦æ›´å¤šclusteræ¥åŒºåˆ†ä¸åŒçš„ç”¨æˆ·ç¾¤ä½“ã€‚"
        )

    # æ‰§è¡Œèšç±»
    if args.clustering_method == "kmeans":
        labels, silhouette = perform_kmeans_clustering(
            user_item_matrix,
            similarity_matrix,
            args.n_clusters,
            compute_silhouette=not args.skip_silhouette,
            silhouette_sample_size=args.silhouette_sample_size,
        )
        linkage_matrix = None
        if args.show_progress:
            if silhouette is not None:
                print(f"K-meansèšç±»å®Œæˆï¼Œè½®å»“ç³»æ•°: {silhouette:.4f}")
            else:
                print(f"K-meansèšç±»å®Œæˆï¼ˆå·²è·³è¿‡è½®å»“ç³»æ•°è®¡ç®—ï¼‰")
    else:
        labels, linkage_matrix = perform_hierarchical_clustering(
            similarity_matrix,
            args.n_clusters,
        )
        # è®¡ç®—è½®å»“ç³»æ•°ï¼ˆå¯é€‰ï¼‰
        silhouette = None
        if not args.skip_silhouette:
            if args.silhouette_sample_size is not None and args.silhouette_sample_size < n_users:
                # é‡‡æ ·è®¡ç®—
                sample_indices = np.random.choice(
                    n_users, size=args.silhouette_sample_size, replace=False
                )
                sample_labels = labels[sample_indices]
                sample_distance = 1 - similarity_matrix[np.ix_(sample_indices, sample_indices)]
                np.fill_diagonal(sample_distance, 0)
                silhouette = silhouette_score(
                    sample_distance, sample_labels, metric="precomputed"
                )
            else:
                # å®Œæ•´è®¡ç®—
                distance_matrix = 1 - similarity_matrix
                np.fill_diagonal(distance_matrix, 0)
                silhouette = silhouette_score(
                    distance_matrix, labels, metric="precomputed"
                )
        if args.show_progress:
            if silhouette is not None:
                print(f"å±‚æ¬¡èšç±»å®Œæˆï¼Œè½®å»“ç³»æ•°: {silhouette:.4f}")
            else:
                print(f"å±‚æ¬¡èšç±»å®Œæˆï¼ˆå·²è·³è¿‡è½®å»“ç³»æ•°è®¡ç®—ï¼‰")

    # åˆ†æèšç±»ç»Ÿè®¡
    cluster_stats = analyze_cluster_statistics(
        user_ids, user_item_matrix, labels, similarity_matrix
    )

    if args.show_progress:
        # è®¡ç®—clusterå¤§å°ç»Ÿè®¡
        cluster_sizes = [stat["size"] for stat in cluster_stats["cluster_statistics"]]
        cluster_sizes_array = np.array(cluster_sizes)
        
        print(f"\nèšç±»åˆ†æç»“æœ:")
        print(f"  èšç±»æ•°é‡: {cluster_stats['n_clusters']}")
        print(f"  æ€»ç”¨æˆ·æ•°: {cluster_stats['total_users']}")
        print(f"  å¹³å‡æ¯ä¸ªclusterç”¨æˆ·æ•°: {np.mean(cluster_sizes_array):.1f}")
        print(f"  Clusterå¤§å°ç»Ÿè®¡: "
              f"æœ€å°={cluster_sizes_array.min()}, "
              f"æœ€å¤§={cluster_sizes_array.max()}, "
              f"ä¸­ä½æ•°={np.median(cluster_sizes_array):.1f}, "
              f"æ ‡å‡†å·®={cluster_sizes_array.std():.1f}")
        
        # ç»Ÿè®¡å°clusterï¼ˆå°‘äº5ä¸ªç”¨æˆ·ï¼‰çš„æ•°é‡
        small_clusters = (cluster_sizes_array < 5).sum()
        if small_clusters > 0:
            print(f"  âš ï¸  å°clusteræ•°é‡ï¼ˆ<5ç”¨æˆ·ï¼‰: {small_clusters} ({small_clusters*100/cluster_stats['n_clusters']:.1f}%)")
        
        print(f"  å¹³å‡ç°‡å†…ç›¸ä¼¼åº¦: {cluster_stats['overall_intra_cluster_similarity']:.4f}")
        print(f"  å¹³å‡ç°‡é—´ç›¸ä¼¼åº¦: {cluster_stats['overall_inter_cluster_similarity']:.4f}")
        print(f"  èšç±»è´¨é‡ (ç°‡å†…-ç°‡é—´): {cluster_stats['clustering_quality']:.4f}")
        if silhouette is not None:
            print(f"  è½®å»“ç³»æ•°: {silhouette:.4f}")
        else:
            print(f"  è½®å»“ç³»æ•°: æœªè®¡ç®—ï¼ˆä½¿ç”¨ --skip-silhouette è·³è¿‡ï¼‰")

        # åªæ˜¾ç¤ºå‰20ä¸ªclusterçš„è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœclusterå¤ªå¤šï¼‰
        max_display = 20
        if len(cluster_stats["cluster_statistics"]) > max_display:
            print(f"\nå„èšç±»ç»Ÿè®¡ï¼ˆæ˜¾ç¤ºå‰{max_display}ä¸ªï¼ŒæŒ‰ç”¨æˆ·æ•°æ’åºï¼‰:")
            sorted_stats = sorted(
                cluster_stats["cluster_statistics"],
                key=lambda x: x["size"],
                reverse=True
            )
            for stat in sorted_stats[:max_display]:
                print(
                    f"  èšç±» {stat['cluster_id']}: "
                    f"ç”¨æˆ·æ•°={stat['size']}, "
                    f"ç°‡å†…ç›¸ä¼¼åº¦={stat['intra_cluster_similarity']:.4f}, "
                    f"ç°‡é—´ç›¸ä¼¼åº¦={stat['inter_cluster_similarity']:.4f}, "
                    f"å”¯ä¸€ç‰©å“æ•°={stat['unique_items']}"
                )
            print(f"  ... (è¿˜æœ‰ {len(cluster_stats['cluster_statistics']) - max_display} ä¸ªclusteræœªæ˜¾ç¤º)")
        else:
            print(f"\nå„èšç±»ç»Ÿè®¡:")
            for stat in cluster_stats["cluster_statistics"]:
                print(
                    f"  èšç±» {stat['cluster_id']}: "
                    f"ç”¨æˆ·æ•°={stat['size']}, "
                    f"ç°‡å†…ç›¸ä¼¼åº¦={stat['intra_cluster_similarity']:.4f}, "
                    f"ç°‡é—´ç›¸ä¼¼åº¦={stat['inter_cluster_similarity']:.4f}, "
                    f"å”¯ä¸€ç‰©å“æ•°={stat['unique_items']}"
                )

    # ä¿å­˜ç»“æœ
    results = {
        "similarity_metric": args.similarity_metric,
        "clustering_method": args.clustering_method,
        "n_clusters": args.n_clusters,
        "silhouette_score": float(silhouette) if silhouette is not None else None,
        "cluster_statistics": cluster_stats,
        "user_cluster_mapping": {
            str(uid): int(label) for uid, label in zip(user_ids, labels)
        },
    }

    results_json_path = args.output_dir / "user_clustering_results.json"
    results_json_path.write_text(
        json.dumps(results, indent=2, ensure_ascii=False)
    )
    if args.show_progress:
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_json_path}")

    # ç»˜åˆ¶å¯è§†åŒ–
    if plt is not None:
        # ç›¸ä¼¼åº¦çƒ­åŠ›å›¾
        heatmap_path = args.output_dir / "user_similarity_heatmap.png"
        plot_similarity_heatmap(
            similarity_matrix,
            heatmap_path,
            labels=labels,
            max_users=args.max_heatmap_users,
        )
        if args.show_progress:
            print(f"ç›¸ä¼¼åº¦çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {heatmap_path}")

        # èšç±»åˆ†å¸ƒ
        cluster_dist_path = args.output_dir / "cluster_distribution.png"
        plot_cluster_distribution(labels, similarity_matrix, cluster_dist_path)
        if args.show_progress:
            print(f"èšç±»åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {cluster_dist_path}")

        # å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾ï¼ˆå¦‚æœä½¿ç”¨å±‚æ¬¡èšç±»ï¼‰
        if linkage_matrix is not None:
            dendrogram_path = args.output_dir / "dendrogram.png"
            plot_dendrogram(linkage_matrix, dendrogram_path)
            if args.show_progress:
                print(f"æ ‘çŠ¶å›¾å·²ä¿å­˜åˆ°: {dendrogram_path}")

    # ä¿å­˜ç”¨æˆ·èšç±»æ˜ å°„CSV
    user_cluster_df = pd.DataFrame({
        "user_id": user_ids,
        "cluster_id": labels,
    })
    user_cluster_csv_path = args.output_dir / "user_cluster_mapping.csv"
    user_cluster_df.to_csv(user_cluster_csv_path, index=False)
    if args.show_progress:
        print(f"ç”¨æˆ·èšç±»æ˜ å°„å·²ä¿å­˜åˆ°: {user_cluster_csv_path}")


if __name__ == "__main__":
    main()

